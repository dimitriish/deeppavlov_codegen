{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# model_id = \"Salesforce/codegen25-7b-mono\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab89b70856a24789a9992bfafe5960e7"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "code_snippet = \"\"\"def svg_to_image(string, size=None):\n",
    "    if isinstance(string, unicode):\n",
    "        string = string.encode('utf-8')\n",
    "    renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n",
    "    if not renderer.isValid():\n",
    "        raise ValueError('Invalid SVG data.')\n",
    "    if size is None:\n",
    "        size = renderer.defaultSize()\n",
    "    image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n",
    "    painter = QtGui.QPainter(image)\n",
    "    renderer.render(painter)\n",
    "    return image\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "prompt = \"Write a summary for the following code in natural language:\\n\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sequences = pipeline(\n",
    "    prompt + code_snippet,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.3,\n",
    "    num_return_sequences=1,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sir_timio/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 165, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result: Write a summary for the following code in natural language:\n",
      "def svg_to_image(string, size=None):\n",
      "    if isinstance(string, unicode):\n",
      "        string = string.encode('utf-8')\n",
      "    renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n",
      "    if not renderer.isValid():\n",
      "        raise ValueError('Invalid SVG data.')\n",
      "    if size is None:\n",
      "        size = renderer.defaultSize()\n",
      "    image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n",
      "    painter = QtGui.QPainter(image)\n",
      "    renderer.render(painter)\n",
      "    return image\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "code_snippet = \"\"\"\n",
    "def history_to_hash(npz: Union[None, str, FullGeneration]):\n",
    "    if npz is None:\n",
    "        return get_md5_hex(b\"None\")\n",
    "    if isinstance(npz, str):\n",
    "        return get_hash_from_voice_name(npz)\n",
    "    npz_str = json.dumps(\n",
    "        {\n",
    "            \"semantic_prompt\": npz[\"semantic_prompt\"].tolist(),\n",
    "            \"coarse_prompt\": npz[\"coarse_prompt\"].tolist(),\n",
    "            \"fine_prompt\": npz[\"fine_prompt\"].tolist(),\n",
    "        }\n",
    "    )\n",
    "    npz_as_str = npz_str.encode(\"utf-8\")\n",
    "    return get_md5_hex(npz_as_str)\n",
    "\"\"\"\n",
    "sequences = pipeline(\n",
    "    prompt + code_snippet,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.3,\n",
    "    num_return_sequences=1,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result: Write a summary for the following code in natural language:\n",
      "\n",
      "def history_to_hash(npz: Union[None, str, FullGeneration]):\n",
      "    if npz is None:\n",
      "        return get_md5_hex(b\"None\")\n",
      "    if isinstance(npz, str):\n",
      "        return get_hash_from_voice_name(npz)\n",
      "    npz_str = json.dumps(\n",
      "        {\n",
      "            \"semantic_prompt\": npz[\"semantic_prompt\"].tolist(),\n",
      "            \"coarse_prompt\": npz[\"coarse_prompt\"].tolist(),\n",
      "            \"fine_prompt\": npz[\"fine_prompt\"].tolist(),\n",
      "        }\n",
      "    )\n",
      "    npz_as_str = npz_str.encode(\"utf-8\")\n",
      "    return get_md5_hex(npz_as_str)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "code_snippet = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "sequences = pipeline(\n",
    "    prompt + code_snippet,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.3,\n",
    "    num_return_sequences=1,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result: Write a summary for the following code in natural language:\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "Answer: This is a recursive function that calculates the nth Fibonacci number.\n",
      "\n",
      "Answer: This is a recursive function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "sequences = pipeline(\n",
    "    prompt + code_snippet,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result: Write a summary for the following code in natural language:\n",
      "class CustomCallbackLLMChain(CustomCallbackChain):\n",
      "    llm: BaseChatModel\n",
      "\n",
      "    def retry_llm(self, messages, retry_count=5):\n",
      "        sleep_duration = 0.5\n",
      "        for _idx in range(retry_count - 1):\n",
      "            try:\n",
      "                return self.llm(messages)\n",
      "            except Exception as e:\n",
      "                self.fire_log(f\"LLM failed with error: {e}; Retrying\")\n",
      "                time.sleep(sleep_duration)\n",
      "            sleep_duration *= 2\n",
      "        return self.llm(messages)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.13 64-bit"
  },
  "interpreter": {
   "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}