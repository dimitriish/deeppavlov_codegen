{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import load_data\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM, SummarizationPipeline, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Salesforce/codet5p-220m-bimodal\"\n",
    "# model_id = \"Salesforce/codet5p-770m-py\"\n",
    "# model_id = \"Salesforce/codet5p-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id,  trust_remote_code=True)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id,\n",
    "#                                               torch_dtype=torch.float16,\n",
    "#                                               trust_remote_code=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_context_in_batches(context, batch_size=20):\n",
    "    context_batches = [context[i:i + batch_size] for i in range(0, len(context), batch_size)]\n",
    "    processed_context = []\n",
    "    for batch in context_batches:\n",
    "        processed_context.extend(code_to_nl_batch(batch))\n",
    "    \n",
    "    return processed_context\n",
    "\n",
    "\n",
    "\n",
    "def code_to_nl_batch(codes: list[str], max_length: int = 100) -> list[str]:\n",
    "    encoded_input = tokenizer(codes, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids = encoded_input.input_ids\n",
    "    generated_ids = model.generate(input_ids, max_length=max_length).cpu()\n",
    "    return [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"def func(string, size=None):\n",
    "if isinstance(string, unicode):\n",
    "    string = string.encode('utf-8')\n",
    "    renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n",
    "if not renderer.isValid():\n",
    "    raise ValueError('Invalid SVG data.')\n",
    "if size is None:\n",
    "    size = renderer.defaultSize()\n",
    "    image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n",
    "    painter = QtGui.QPainter(image)\n",
    "    renderer.render(painter)\n",
    "return image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Render SVG string to a QImage.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "settings = 'cross_file_first'\n",
    "data = load_data('test', 'r', 'python', settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_samples = data['easy']\n",
    "len(raw_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_samples = np.random.choice(raw_samples, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = raw_samples[0]\n",
    "sample = np.random.choice(raw_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['repo_name', 'file_path', 'context', 'import_statement', 'code', 'next_line', 'gold_snippet_index'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['gold_snippet_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def make_coordinate_grid(spatial_size, type):\n",
      "    d, h, w = spatial_size\n",
      "    x = torch.arange(w).type(type)\n",
      "    y = torch.arange(h).type(type)\n",
      "    z = torch.arange(d).type(type)\n",
      "\n",
      "    x = (2 * (x / (w - 1)) - 1)\n",
      "    y = (2 * (y / (h - 1)) - 1)\n",
      "    z = (2 * (z / (d - 1)) - 1)\n",
      "   \n",
      "    yy = y.view(1, -1, 1).repeat(d, 1, w)\n",
      "    xx = x.view(1, 1, -1).repeat(d, h, 1)\n",
      "    zz = z.view(-1, 1, 1).repeat(1, h, w)\n",
      "\n",
      "    meshed = torch.cat([xx.unsqueeze_(3), yy.unsqueeze_(3), zz.unsqueeze_(3)], 3)\n",
      "\n",
      "    return meshed\n"
     ]
    }
   ],
   "source": [
    "print(sample['context'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = sample['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
      "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
      "    of 3d inputs\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      "\n",
      "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
      "    standard-deviation are reduced across all devices during training.\n",
      "\n",
      "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
      "    training, PyTorch's implementation normalize the tensor on each device using\n",
      "    the statistics only on that device, which accelerated the computation and\n",
      "    is also easy to implement, but the statistics might be inaccurate.\n",
      "    Instead, in this synchronized version, the statistics will be computed\n",
      "    over all training samples distributed on multiple devices.\n",
      "    \n",
      "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
      "    as the built-in PyTorch implementation.\n",
      "\n",
      "    The mean and standard-deviation are calculated per-dimension over\n",
      "    the mini-batches and gamma and beta are learnable parameter vectors\n",
      "    of size C (where C is the input size).\n",
      "\n",
      "    During training, this layer keeps a running estimate of its computed mean\n",
      "    and variance. The running sum is kept with a default momentum of 0.1.\n",
      "\n",
      "    During evaluation, this running mean/variance is used for normalization.\n",
      "\n",
      "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
      "\n",
      "    Args:\n",
      "        num_features: num_features from an expected input of\n",
      "            size batch_size x num_features x height x width\n",
      "        eps: a value added to the denominator for numerical stability.\n",
      "            Default: 1e-5\n",
      "        momentum: the value used for the running_mean and running_var\n",
      "            computation. Default: 0.1\n",
      "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      "            affine parameters. Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, C, H, W)`\n",
      "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
      "\n",
      "    Examples:\n",
      "        >>> # With Learnable Parameters\n",
      "        >>> m = SynchronizedBatchNorm2d(100)\n",
      "        >>> # Without Learnable Parameters\n",
      "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
      "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
      "        >>> output = m(input)\n",
      "    \"\"\"\n",
      "\n",
      "    def _check_input_dim(self, input):\n",
      "        if input.dim() != 4:\n",
      "            raise ValueError('expected 4D input (got {}D input)'\n",
      "                             .format(input.dim()))\n",
      "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n"
     ]
    }
   ],
   "source": [
    "print(code_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def batch_norm ( self, num_features, x, num_height, x_width, eps = 1e-5, momentum = 0.1, affine = True ) : if num_features < 1 : raise ValueError ( \"num_features must be greater than 0\" ) if num_height < 1 : raise ValueError ( \"num_height must be greater than 0\" ) if x < 0 : raise ValueError ( \"x must be greater than 0\"']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([code_snippet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_and_remove_docstrings(code):\n",
    "    docstring_pattern = r'(\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\')'\n",
    "    docstrings = re.findall(docstring_pattern, code, flags=re.DOTALL)\n",
    "    cleaned_code = re.sub(docstring_pattern, '', code, flags=re.DOTALL)\n",
    "    return cleaned_code, docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_code_snippet, docstrings = extract_and_remove_docstrings(code_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A SynchronizedBatchNorm2d class to ensure that the input is 4D.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([remove_docstrings(cleaned_code_snippet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00636601448059082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 1206,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8670687598ad42ac95876b318459f8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0061566829681396484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "model.safetensors",
       "rate": null,
       "total": 242043056,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5276f7db82e1416790f67edcdaa67a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006028652191162109,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 147,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b21a09949d4de1861d233075175613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006064176559448242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 2324,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51f2d06e48541c9aad9505184a88998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007505178451538086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "spiece.model",
       "rate": null,
       "total": 791656,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667b3651d1e14b0fa7bb88ab6163881b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00621485710144043,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1389353,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d984294626b4d7bbe033e0c0c160592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", \"t5-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_docstrings(docstrings):\n",
    "    summaries = []\n",
    "    for docstring in docstrings:\n",
    "        summary = summarizer(docstring, max_length=50, min_length=25, do_sample=False)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\'\"Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs . the mean and standard-deviation are reduced across all devices during training . ']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_docstrings(docstrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['repo_name', 'file_path', 'context', 'import_statement', 'code', 'next_line', 'gold_snippet_index'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
      "    of 3d inputs\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      "\n",
      "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
      "    standard-deviation are reduced across all devices during training.\n",
      "\n",
      "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
      "    training, PyTorch's implementation normalize the tensor on each device using\n",
      "    the statistics only on that device, which accelerated the computation and\n",
      "    is also easy to implement, but the statistics might be inaccurate.\n",
      "    Instead, in this synchronized version, the statistics will be computed\n",
      "    over all training samples distributed on multiple devices.\n",
      "    \n",
      "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
      "    as the built-in PyTorch implementation.\n",
      "\n",
      "    The mean and standard-deviation are calculated per-dimension over\n",
      "    the mini-batches and gamma and beta are learnable parameter vectors\n",
      "    of size C (where C is the input size).\n",
      "\n",
      "    During training, this layer keeps a running estimate of its computed mean\n",
      "    and variance. The running sum is kept with a default momentum of 0.1.\n",
      "\n",
      "    During evaluation, this running mean/variance is used for normalization.\n",
      "\n",
      "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
      "\n",
      "    Args:\n",
      "        num_features: num_features from an expected input of\n",
      "            size batch_size x num_features x height x width\n",
      "        eps: a value added to the denominator for numerical stability.\n",
      "            Default: 1e-5\n",
      "        momentum: the value used for the running_mean and running_var\n",
      "            computation. Default: 0.1\n",
      "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      "            affine parameters. Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, C, H, W)`\n",
      "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
      "\n",
      "    Examples:\n",
      "        >>> # With Learnable Parameters\n",
      "        >>> m = SynchronizedBatchNorm2d(100)\n",
      "        >>> # Without Learnable Parameters\n",
      "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
      "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
      "        >>> output = m(input)\n",
      "    \"\"\"\n"
     ]
    }
   ],
   "source": [
    "print(docstrings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "class KPDetector(nn.Module):\n",
      "    \"\"\"\n",
      "    Detecting canonical keypoints. Return keypoint position and jacobian near each keypoint.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, block_expansion, feature_channel, num_kp, image_channel, max_features, reshape_channel, reshape_depth,\n",
      "                 num_blocks, temperature, estimate_jacobian=False, scale_factor=1, single_jacobian_map=False):\n",
      "        super(KPDetector, self).__init__()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['repo_name', 'file_path', 'context', 'import_statement', 'code', 'next_line', 'gold_snippet_index'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from torch import nn\n",
      "from src.pretrained.face_vid2vid.sync_batchnorm import SynchronizedBatchNorm2d as BatchNorm2d\n",
      "from src.pretrained.face_vid2vid.modules.util import KPHourglass, make_coordinate_grid, AntiAliasInterpolation2d, ResBottleneck\n",
      "import torch\n",
      "import torch.nn.functional as F\n"
     ]
    }
   ],
   "source": [
    "print(sample['import_statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'src/pretrained/face_vid2vid/modules/keypoint_detector.py'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next line -> multi line\n",
    "# code + ml -> describe ml without keywords\n",
    "# golden context <-> nl ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        self.predictor = KPHourglass(block_expansion, in_features=image_channel,'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['next_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {[()]} + ' + \" + \\+ + \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'src/pretrained/face_vid2vid/modules/keypoint_detector.py'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "from torch import nn\n",
    "from src.pretrained.face_vid2vid.sync_batchnorm import SynchronizedBatchNorm2d as BatchNorm2d\n",
    "from src.pretrained.face_vid2vid.modules.util import KPHourglass, make_coordinate_grid, AntiAliasInterpolation2d, ResBottleneck\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "        self.predictor = KPHourglass(block_expansion, in_features=image_channel,\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.index(\"self.predictor = KPHourglass(block_expansion, in_features=image_channel,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class KPHourglass(nn.Module):\n",
      "    \"\"\"\n",
      "    Hourglass architecture.\n",
      "    \"\"\" \n",
      "\n",
      "    def __init__(self, block_expansion, in_features, reshape_features, reshape_depth, num_blocks=3, max_features=256):\n",
      "        super(KPHourglass, self).__init__()\n",
      "        \n",
      "        self.down_blocks = nn.Sequential()\n",
      "        for i in range(num_blocks):\n",
      "            self.down_blocks.add_module('down'+ str(i), DownBlock2d(in_features if i == 0 else min(max_features, block_expansion * (2 ** i)),\n",
      "                                                                   min(max_features, block_expansion * (2 ** (i + 1))),\n",
      "                                                                   kernel_size=3, padding=1))\n",
      "\n",
      "        in_filters = min(max_features, block_expansion * (2 ** num_blocks))\n",
      "        self.conv = nn.Conv2d(in_channels=in_filters, out_channels=reshape_features, kernel_size=1)\n",
      "\n",
      "        self.up_blocks = nn.Sequential()\n",
      "        for i in range(num_blocks):\n",
      "            in_filters = min(max_features, block_expansion * (2 ** (num_blocks - i)))\n",
      "            out_filters = min(max_features, block_expansion * (2 ** (num_blocks - i - 1)))\n",
      "            self.up_blocks.add_module('up'+ str(i), UpBlock3d(in_filters, out_filters, kernel_size=3, padding=1))\n",
      "\n",
      "        self.reshape_depth = reshape_depth\n",
      "        self.out_filters = out_filters\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.down_blocks(x)\n",
      "        out = self.conv(out)\n",
      "        bs, c, h, w = out.shape\n",
      "        out = out.view(bs, c//self.reshape_depth, self.reshape_depth, h, w)\n",
      "        out = self.up_blocks(out)\n",
      "\n",
      "        return out\n"
     ]
    }
   ],
   "source": [
    "print(sample['context'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KPHourglass architecture.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([sample['context'][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give detailed description for following code:        self.predictor = KPHourglass(block_expansion, in_features=image_channel,'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Give detailed description for following code:\" + sample['next_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "class KPDetector(nn.Module):\n",
      "    \"\"\"\n",
      "    Detecting canonical keypoints. Return keypoint position and jacobian near each keypoint.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, block_expansion, feature_channel, num_kp, image_channel, max_features, reshape_channel, reshape_depth,\n",
      "                 num_blocks, temperature, estimate_jacobian=False, scale_factor=1, single_jacobian_map=False):\n",
      "        super(KPDetector, self).__init__()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "code, docs = extract_and_remove_docstrings(sample['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def detect_canonical_keypoints ( self ) : self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input ( ) self. _check_input (']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([sample['code']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def __init__ ( self, block_expansion, feature_channel, max_features, reshape_channel, reshape_depth, estimate_jacobian = False, scale_factor = 1, single_jacobian_map = False ) : super ( KPDetector, self ). __init__ ( block_expansion, feature_channel, max_features, reshape_channel, reshape_depth, estimate_jacobian, scale_factor, single_jacobian']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def predictor ( self, image_channel, in_features, * * kwargs ) : if not self. _predictor : self. _predictor = KPHourglass ( image_channel, in_features, * * kwargs ) return self. _predictor']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([sample['next_line']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 50, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"\"\" Detecting canonical keypoints . return keypoint position and jacobian near each keypoint .']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_docstrings(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def predictor ( self, image_channel, in_features = None ) : if in_features is None : in_features = self. image_channels if in_features is None : in_features = self. image_channels if self. predictor is None : self. predictor = KPHourglass ( image_channel, in_features ) return self. predictor']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_nl_batch([\"Give detailed description for following code:\\n\" + sample['next_line']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, sample in tqdm(enumerate(raw_samples), total=len(raw_samples)):\n",
    "    sample['nl_code'] = code_to_nl_batch([sample['code']])[0]\n",
    "    sample['nl_context'] = code_to_nl_batch(sample['context'])\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3585"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(sample['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This function returns a function that returns the tokenized answer spans that better match the annotated answer.\n",
      "lse\n",
      "\n",
      "\n",
      "def squad_convert_example_to_features(\n",
      "    example, max_seq_length, doc_stride, max_query_length, padding_strategy, is_training\n",
      "):\n",
      "    features = []\n",
      "    if is_training and not example.is_impossible:\n",
      "        # Get start and end position\n",
      "        start_position = example.start_position\n",
      "        end_position = example.end_position\n",
      "\n",
      "        # If the answer cannot be found in the text, then skip this example.\n",
      "        actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample['nl_code'])\n",
    "print(sample['code'][-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns True if the TTF language generator is available.\n",
      "def is_tf_available():\n",
      "    return _tf_available\n",
      "**************************************************\n",
      "Returns True if the torch is available.\n",
      "def is_torch_available():\n",
      "    return _torch_available\n",
      "**************************************************\n",
      "Runs basic whitespace cleaning and splitting on a piece of text.\n",
      "def whitespace_tokenize(text):\n",
      "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
      "    text = text.strip()\n",
      "    if not text:\n",
      "        return []\n",
      "    tokens = text.split()\n",
      "    return tokens\n",
      "**************************************************\n",
      "This class is derived from a dictionary and can be used as a base class for batch encoding.\n",
      "class BatchEncoding(UserDict):\n",
      "    \"\"\"\n",
      "    Holds the output of the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus` and\n",
      "    :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode` methods (tokens,\n",
      "    attention_masks, etc).\n",
      "\n",
      "    This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes\n",
      "    utility methods to map from word/character space to token space.\n",
      "\n",
      "    Args:\n",
      "        data (:obj:`dict`):\n",
      "            Dictionary of lists/arrays/tensors returned by the encode/batch_encode methods ('input_ids',\n",
      "            'attention_mask', etc.).\n",
      "        encoding (:obj:`tokenizers.Encoding` or :obj:`Sequence[tokenizers.Encoding]`, `optional`):\n",
      "            If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character\n",
      "            space to token space the :obj:`tokenizers.Encoding` instance or list of instance (for batches) hold this\n",
      "            information.\n",
      "        tensor_type (:obj:`Union[None, str, TensorType]`, `optional`):\n",
      "            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
      "            initialization.\n",
      "        prepend_batch_axis (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to add a batch axis when converting to tensors (see :obj:`tensor_type` above).\n",
      "        n_sequences (:obj:`Optional[int]`, `optional`):\n",
      "            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
      "            initialization.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        data: Optional[Dict[str, Any]] = None,\n",
      "        encoding: Optional[Union[EncodingFast, Sequence[EncodingFast]]] = None,\n",
      "        tensor_type: Union[None, str, TensorType] = None,\n",
      "        prepend_batch_axis: bool = False,\n",
      "        n_sequences: Optional[int] = None,\n",
      "    ):\n",
      "        super().__init__(data)\n",
      "\n",
      "        if isinstance(encoding, EncodingFast):\n",
      "            encoding = [encoding]\n",
      "\n",
      "        self._encodings = encoding\n",
      "\n",
      "        if n_sequences is None and encoding is not None and len(encoding):\n",
      "            n_sequences = encoding[0].n_sequences\n",
      "\n",
      "        self._n_sequences = n_sequences\n",
      "\n",
      "        self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "    @property\n",
      "    def n_sequences(self) -> Optional[int]:\n",
      "        \"\"\"\n",
      "        :obj:`Optional[int]`: The number of sequences used to generate each sample from the batch encoded in this\n",
      "        :class:`~transformers.BatchEncoding`. Currently can be one of :obj:`None` (unknown), :obj:`1` (a single\n",
      "        sentence) or :obj:`2` (a pair of sentences)\n",
      "        \"\"\"\n",
      "        return self._n_sequences\n",
      "\n",
      "    @property\n",
      "    def is_fast(self) -> bool:\n",
      "        \"\"\"\n",
      "        :obj:`bool`: Indicate whether this :class:`~transformers.BatchEncoding` was generated from the result of a\n",
      "        :class:`~transformers.PreTrainedTokenizerFast` or not.\n",
      "        \"\"\"\n",
      "        return self._encodings is not None\n",
      "\n",
      "    def __getitem__(self, item: Union[int, str]) -> Union[Any, EncodingFast]:\n",
      "        \"\"\"\n",
      "        If the key is a string, returns the value of the dict associated to :obj:`key` ('input_ids', 'attention_mask',\n",
      "        etc.).\n",
      "\n",
      "        If the key is an integer, get the :obj:`tokenizers.Encoding` for batch item with index :obj:`key`.\n",
      "        \"\"\"\n",
      "        if isinstance(item, str):\n",
      "            return self.data[item]\n",
      "        elif self._encodings is not None:\n",
      "            return self._encodings[item]\n",
      "        else:\n",
      "            raise KeyError(\n",
      "                \"Indexing with integers (to access backend Encoding for a given batch index) \"\n",
      "                \"is not available when using Python based tokenizers\"\n",
      "            )\n",
      "\n",
      "    def __getattr__(self, item: str):\n",
      "        try:\n",
      "            return self.data[item]\n",
      "        except KeyError:\n",
      "            raise AttributeError\n",
      "\n",
      "    def __getstate__(self):\n",
      "        return {\"data\": self.data, \"encodings\": self._encodings}\n",
      "\n",
      "    def __setstate__(self, state):\n",
      "        if \"data\" in state:\n",
      "            self.data = state[\"data\"]\n",
      "\n",
      "        if \"encodings\" in state:\n",
      "            self._encodings = state[\"encodings\"]\n",
      "\n",
      "    def keys(self):\n",
      "        return self.data.keys()\n",
      "\n",
      "    def values(self):\n",
      "        return self.data.values()\n",
      "\n",
      "    def items(self):\n",
      "        return self.data.items()\n",
      "\n",
      "    # After this point:\n",
      "    # Extended properties and methods only available for fast (Rust-based) tokenizers\n",
      "    # provided by HuggingFace tokenizers library.\n",
      "\n",
      "    @property\n",
      "    def encodings(self) -> Optional[List[EncodingFast]]:\n",
      "        \"\"\"\n",
      "        :obj:`Optional[List[tokenizers.Encoding]]`: The list all encodings from the tokenization process. Returns\n",
      "        :obj:`None` if the input was tokenized through Python (i.e., not a fast) tokenizer.\n",
      "        \"\"\"\n",
      "        return self._encodings\n",
      "\n",
      "    def tokens(self, batch_index: int = 0) -> List[str]:\n",
      "        \"\"\"\n",
      "        Return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to\n",
      "        integer indices) at a given batch index (only works for the output of a fast tokenizer).\n",
      "\n",
      "        Args:\n",
      "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[str]`: The list of tokens at that index.\n",
      "        \"\"\"\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"tokens() is not available when using Python-based tokenizers\")\n",
      "        return self._encodings[batch_index].tokens\n",
      "\n",
      "    def sequence_ids(self, batch_index: int = 0) -> List[Optional[int]]:\n",
      "        \"\"\"\n",
      "        Return a list mapping the tokens to the id of their original sentences:\n",
      "\n",
      "            - :obj:`None` for special tokens added around or between sequences,\n",
      "            - :obj:`0` for tokens corresponding to words in the first sequence,\n",
      "            - :obj:`1` for tokens corresponding to words in the second sequence when a pair of sequences was jointly\n",
      "              encoded.\n",
      "\n",
      "        Args:\n",
      "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[Optional[int]]`: A list indicating the sequence id corresponding to each token. Special tokens\n",
      "            added by the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their\n",
      "            corresponding sequence.\n",
      "        \"\"\"\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"sequence_ids() is not available when using Python-based tokenizers\")\n",
      "        return self._encodings[batch_index].sequence_ids\n",
      "\n",
      "    def words(self, batch_index: int = 0) -> List[Optional[int]]:\n",
      "        \"\"\"\n",
      "        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\n",
      "\n",
      "        Args:\n",
      "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by\n",
      "            the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their corresponding\n",
      "            word (several tokens will be mapped to the same word index if they are parts of that word).\n",
      "        \"\"\"\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"words() is not available when using Python-based tokenizers\")\n",
      "        warnings.warn(\n",
      "            \"`BatchEncoding.words()` property is deprecated and should be replaced with the identical, \"\n",
      "            \"but more self-explanatory `BatchEncoding.word_ids()` property.\",\n",
      "            FutureWarning,\n",
      "        )\n",
      "        return self.word_ids(batch_index)\n",
      "\n",
      "    def word_ids(self, batch_index: int = 0) -> List[Optional[int]]:\n",
      "        \"\"\"\n",
      "        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\n",
      "\n",
      "        Args:\n",
      "            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by\n",
      "            the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their corresponding\n",
      "            word (several tokens will be mapped to the same word index if they are parts of that word).\n",
      "        \"\"\"\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"word_ids() is not available when using Python-based tokenizers\")\n",
      "        return self._encodings[batch_index].word_ids\n",
      "\n",
      "    def token_to_sequence(self, batch_or_token_index: int, token_index: Optional[int] = None) -> int:\n",
      "        \"\"\"\n",
      "        Get the index of the sequence represented by the given token. In the general use case, this method returns\n",
      "        :obj:`0` for a single sequence or the first sequence of a pair, and :obj:`1` for the second sequence of a pair\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.token_to_sequence(token_index)`` if batch size is 1\n",
      "        - ``self.token_to_sequence(batch_index, token_index)`` if batch size is greater than 1\n",
      "\n",
      "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,\n",
      "        words are defined by the user). In this case it allows to easily associate encoded tokens with provided\n",
      "        tokenized words.\n",
      "\n",
      "        Args:\n",
      "            batch_or_token_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of\n",
      "                the token in the sequence.\n",
      "            token_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the token in the\n",
      "                sequence.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`int`: Index of the word in the input sequence.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"token_to_sequence() is not available when using Python based tokenizers\")\n",
      "        if token_index is not None:\n",
      "            batch_index = batch_or_token_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            token_index = batch_or_token_index\n",
      "        if batch_index < 0:\n",
      "            batch_index = self._batch_size + batch_index\n",
      "        if token_index < 0:\n",
      "            token_index = self._seq_len + token_index\n",
      "        return self._encodings[batch_index].token_to_sequence(token_index)\n",
      "\n",
      "    def token_to_word(self, batch_or_token_index: int, token_index: Optional[int] = None) -> int:\n",
      "        \"\"\"\n",
      "        Get the index of the word corresponding (i.e. comprising) to an encoded token in a sequence of the batch.\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.token_to_word(token_index)`` if batch size is 1\n",
      "        - ``self.token_to_word(batch_index, token_index)`` if batch size is greater than 1\n",
      "\n",
      "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,\n",
      "        words are defined by the user). In this case it allows to easily associate encoded tokens with provided\n",
      "        tokenized words.\n",
      "\n",
      "        Args:\n",
      "            batch_or_token_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
      "                the token in the sequence.\n",
      "            token_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the token in the\n",
      "                sequence.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`int`: Index of the word in the input sequence.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"token_to_word() is not available when using Python based tokenizers\")\n",
      "        if token_index is not None:\n",
      "            batch_index = batch_or_token_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            token_index = batch_or_token_index\n",
      "        if batch_index < 0:\n",
      "            batch_index = self._batch_size + batch_index\n",
      "        if token_index < 0:\n",
      "            token_index = self._seq_len + token_index\n",
      "        return self._encodings[batch_index].token_to_word(token_index)\n",
      "\n",
      "    def word_to_tokens(\n",
      "        self, batch_or_word_index: int, word_index: Optional[int] = None, sequence_index: int = 0\n",
      "    ) -> Optional[TokenSpan]:\n",
      "        \"\"\"\n",
      "        Get the encoded token span corresponding to a word in a sequence of the batch.\n",
      "\n",
      "        Token spans are returned as a :class:`~transformers.tokenization_utils_base.TokenSpan` with:\n",
      "\n",
      "        - **start** -- Index of the first token.\n",
      "        - **end** -- Index of the token following the last token.\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.word_to_tokens(word_index, sequence_index: int = 0)`` if batch size is 1\n",
      "        - ``self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)`` if batch size is greater or equal\n",
      "          to 1\n",
      "\n",
      "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words\n",
      "        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized\n",
      "        words.\n",
      "\n",
      "        Args:\n",
      "            batch_or_word_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of\n",
      "                the word in the sequence.\n",
      "            word_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the\n",
      "                sequence.\n",
      "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
      "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
      "                or 1) the provided word index belongs to.\n",
      "\n",
      "        Returns:\n",
      "            Optional :class:`~transformers.tokenization_utils_base.TokenSpan` Span of tokens in the encoded sequence.\n",
      "            Returns :obj:`None` if no tokens correspond to the word.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"word_to_tokens() is not available when using Python based tokenizers\")\n",
      "        if word_index is not None:\n",
      "            batch_index = batch_or_word_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            word_index = batch_or_word_index\n",
      "        if batch_index < 0:\n",
      "            batch_index = self._batch_size + batch_index\n",
      "        if word_index < 0:\n",
      "            word_index = self._seq_len + word_index\n",
      "        span = self._encodings[batch_index].word_to_tokens(word_index, sequence_index)\n",
      "        return TokenSpan(*span) if span is not None else None\n",
      "\n",
      "    def token_to_chars(self, batch_or_token_index: int, token_index: Optional[int] = None) -> CharSpan:\n",
      "        \"\"\"\n",
      "        Get the character span corresponding to an encoded token in a sequence of the batch.\n",
      "\n",
      "        Character spans are returned as a :class:`~transformers.tokenization_utils_base.CharSpan` with:\n",
      "\n",
      "        - **start** -- Index of the first character in the original string associated to the token.\n",
      "        - **end** -- Index of the character following the last character in the original string associated to the\n",
      "          token.\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.token_to_chars(token_index)`` if batch size is 1\n",
      "        - ``self.token_to_chars(batch_index, token_index)`` if batch size is greater or equal to 1\n",
      "\n",
      "        Args:\n",
      "            batch_or_token_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
      "                the token in the sequence.\n",
      "            token_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the token or tokens in\n",
      "                the sequence.\n",
      "\n",
      "        Returns:\n",
      "            :class:`~transformers.tokenization_utils_base.CharSpan`: Span of characters in the original string.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"token_to_chars() is not available when using Python based tokenizers\")\n",
      "        if token_index is not None:\n",
      "            batch_index = batch_or_token_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            token_index = batch_or_token_index\n",
      "        return CharSpan(*(self._encodings[batch_index].token_to_chars(token_index)))\n",
      "\n",
      "    def char_to_token(\n",
      "        self, batch_or_char_index: int, char_index: Optional[int] = None, sequence_index: int = 0\n",
      "    ) -> int:\n",
      "        \"\"\"\n",
      "        Get the index of the token in the encoded output comprising a character in the original string for a sequence\n",
      "        of the batch.\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.char_to_token(char_index)`` if batch size is 1\n",
      "        - ``self.char_to_token(batch_index, char_index)`` if batch size is greater or equal to 1\n",
      "\n",
      "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words\n",
      "        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized\n",
      "        words.\n",
      "\n",
      "        Args:\n",
      "            batch_or_char_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
      "                the word in the sequence\n",
      "            char_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the\n",
      "                sequence.\n",
      "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
      "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
      "                or 1) the provided character index belongs to.\n",
      "\n",
      "\n",
      "        Returns:\n",
      "            :obj:`int`: Index of the token.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"char_to_token() is not available when using Python based tokenizers\")\n",
      "        if char_index is not None:\n",
      "            batch_index = batch_or_char_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            char_index = batch_or_char_index\n",
      "        return self._encodings[batch_index].char_to_token(char_index, sequence_index)\n",
      "\n",
      "    def word_to_chars(\n",
      "        self, batch_or_word_index: int, word_index: Optional[int] = None, sequence_index: int = 0\n",
      "    ) -> CharSpan:\n",
      "        \"\"\"\n",
      "        Get the character span in the original string corresponding to given word in a sequence of the batch.\n",
      "\n",
      "        Character spans are returned as a CharSpan NamedTuple with:\n",
      "\n",
      "        - start: index of the first character in the original string\n",
      "        - end: index of the character following the last character in the original string\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.word_to_chars(word_index)`` if batch size is 1\n",
      "        - ``self.word_to_chars(batch_index, word_index)`` if batch size is greater or equal to 1\n",
      "\n",
      "        Args:\n",
      "            batch_or_word_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
      "                the word in the sequence\n",
      "            word_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the\n",
      "                sequence.\n",
      "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
      "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
      "                or 1) the provided word index belongs to.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`CharSpan` or :obj:`List[CharSpan]`: Span(s) of the associated character or characters in the string.\n",
      "            CharSpan are NamedTuple with:\n",
      "\n",
      "                - start: index of the first character associated to the token in the original string\n",
      "                - end: index of the character following the last character associated to the token in the original\n",
      "                  string\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"word_to_chars() is not available when using Python based tokenizers\")\n",
      "        if word_index is not None:\n",
      "            batch_index = batch_or_word_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            word_index = batch_or_word_index\n",
      "        return CharSpan(*(self._encodings[batch_index].word_to_chars(word_index, sequence_index)))\n",
      "\n",
      "    def char_to_word(self, batch_or_char_index: int, char_index: Optional[int] = None, sequence_index: int = 0) -> int:\n",
      "        \"\"\"\n",
      "        Get the word in the original string corresponding to a character in the original string of a sequence of the\n",
      "        batch.\n",
      "\n",
      "        Can be called as:\n",
      "\n",
      "        - ``self.char_to_word(char_index)`` if batch size is 1\n",
      "        - ``self.char_to_word(batch_index, char_index)`` if batch size is greater than 1\n",
      "\n",
      "        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words\n",
      "        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized\n",
      "        words.\n",
      "\n",
      "        Args:\n",
      "            batch_or_char_index (:obj:`int`):\n",
      "                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of\n",
      "                the character in the original string.\n",
      "            char_index (:obj:`int`, `optional`):\n",
      "                If a batch index is provided in `batch_or_token_index`, this can be the index of the character in the\n",
      "                original string.\n",
      "            sequence_index (:obj:`int`, `optional`, defaults to 0):\n",
      "                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0\n",
      "                or 1) the provided character index belongs to.\n",
      "\n",
      "\n",
      "        Returns:\n",
      "            :obj:`int` or :obj:`List[int]`: Index or indices of the associated encoded token(s).\n",
      "        \"\"\"\n",
      "\n",
      "        if not self._encodings:\n",
      "            raise ValueError(\"char_to_word() is not available when using Python based tokenizers\")\n",
      "        if char_index is not None:\n",
      "            batch_index = batch_or_char_index\n",
      "        else:\n",
      "            batch_index = 0\n",
      "            char_index = batch_or_char_index\n",
      "        return self._encodings[batch_index].char_to_word(char_index, sequence_index)\n",
      "\n",
      "    def convert_to_tensors(\n",
      "        self, tensor_type: Optional[Union[str, TensorType]] = None, prepend_batch_axis: bool = False\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Convert the inner content to tensors.\n",
      "\n",
      "        Args:\n",
      "            tensor_type (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      "                The type of tensors to use. If :obj:`str`, should be one of the values of the enum\n",
      "                :class:`~transformers.file_utils.TensorType`. If :obj:`None`, no modification is done.\n",
      "            prepend_batch_axis (:obj:`int`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to add the batch dimension during the conversion.\n",
      "        \"\"\"\n",
      "        if tensor_type is None:\n",
      "            return self\n",
      "\n",
      "        # Convert to TensorType\n",
      "        if not isinstance(tensor_type, TensorType):\n",
      "            tensor_type = TensorType(tensor_type)\n",
      "\n",
      "        # Get a function reference for the correct framework\n",
      "        if tensor_type == TensorType.TENSORFLOW:\n",
      "            if not is_tf_available():\n",
      "                raise ImportError(\n",
      "                    \"Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.\"\n",
      "                )\n",
      "            import tensorflow as tf\n",
      "\n",
      "            as_tensor = tf.constant\n",
      "            is_tensor = tf.is_tensor\n",
      "        elif tensor_type == TensorType.PYTORCH:\n",
      "            if not is_torch_available():\n",
      "                raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n",
      "            import torch\n",
      "\n",
      "            as_tensor = torch.tensor\n",
      "            is_tensor = torch.is_tensor\n",
      "        elif tensor_type == TensorType.JAX:\n",
      "            if not is_flax_available():\n",
      "                raise ImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")\n",
      "            import jax.numpy as jnp  # noqa: F811\n",
      "\n",
      "            as_tensor = jnp.array\n",
      "            is_tensor = _is_jax\n",
      "        else:\n",
      "            as_tensor = np.asarray\n",
      "            is_tensor = _is_numpy\n",
      "        # (mfuntowicz: This code is unreachable)\n",
      "        # else:\n",
      "        #     raise ImportError(\n",
      "        #         \"Unable to convert output to tensors format {}\".format(tensor_type)\n",
      "        #     )\n",
      "\n",
      "        # Do the tensor conversion in batch\n",
      "        for key, value in self.items():\n",
      "            try:\n",
      "                if prepend_batch_axis:\n",
      "                    value = [value]\n",
      "\n",
      "                if not is_tensor(value):\n",
      "                    tensor = as_tensor(value)\n",
      "\n",
      "                    # Removing this for now in favor of controlling the shape with `prepend_batch_axis`\n",
      "                    # # at-least2d\n",
      "                    # if tensor.ndim > 2:\n",
      "                    #     tensor = tensor.squeeze(0)\n",
      "                    # elif tensor.ndim < 2:\n",
      "                    #     tensor = tensor[None, :]\n",
      "\n",
      "                    self[key] = tensor\n",
      "            except:  # noqa E722\n",
      "                if key == \"overflowing_tokens\":\n",
      "                    raise ValueError(\n",
      "                        \"Unable to create tensor returning overflowing tokens of different lengths. \"\n",
      "                        \"Please see if a fast version of this tokenizer is available to have this feature available.\"\n",
      "                    )\n",
      "                raise ValueError(\n",
      "                    \"Unable to create tensor, you should probably activate truncation and/or padding \"\n",
      "                    \"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\n",
      "                )\n",
      "\n",
      "        return self\n",
      "\n",
      "    @torch_required\n",
      "    def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":\n",
      "        \"\"\"\n",
      "        Send all values to device by calling :obj:`v.to(device)` (PyTorch only).\n",
      "\n",
      "        Args:\n",
      "            device (:obj:`str` or :obj:`torch.device`): The device to put the tensors on.\n",
      "\n",
      "        Returns:\n",
      "            :class:`~transformers.BatchEncoding`: The same instance after modification.\n",
      "        \"\"\"\n",
      "\n",
      "        # This check catches things like APEX blindly calling \"to\" on all inputs to a module\n",
      "        # Otherwise it passes the casts down and casts the LongTensor containing the token idxs\n",
      "        # into a HalfTensor\n",
      "        if isinstance(device, str) or _is_torch_device(device) or isinstance(device, int):\n",
      "            self.data = {k: v.to(device=device) for k, v in self.data.items()}\n",
      "        else:\n",
      "            logger.warning(f\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\")\n",
      "        return self\n",
      "**************************************************\n",
      "Base class for pre - trained tokenization.\n",
      "class PreTrainedTokenizerBase(SpecialTokensMixin):\n",
      "    \"\"\"\n",
      "    Base class for :class:`~transformers.PreTrainedTokenizer` and :class:`~transformers.PreTrainedTokenizerFast`.\n",
      "\n",
      "    Handles shared (mostly boiler plate) methods for those two classes.\n",
      "    \"\"\"\n",
      "\n",
      "    vocab_files_names: Dict[str, str] = {}\n",
      "    pretrained_vocab_files_map: Dict[str, Dict[str, str]] = {}\n",
      "    pretrained_init_configuration: Dict[str, Dict[str, Any]] = {}\n",
      "    max_model_input_sizes: Dict[str, Optional[int]] = {}\n",
      "\n",
      "    # first name has to correspond to main model input name\n",
      "    # to make sure `tokenizer.pad(...)` works correctly\n",
      "    model_input_names: List[str] = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
      "    padding_side: str = \"right\"\n",
      "    slow_tokenizer_class = None\n",
      "\n",
      "    def __init__(self, **kwargs):\n",
      "        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)\n",
      "        self.init_inputs = ()\n",
      "        self.init_kwargs = copy.deepcopy(kwargs)\n",
      "        self.name_or_path = kwargs.pop(\"name_or_path\", \"\")\n",
      "\n",
      "        # For backward compatibility we fallback to set model_max_length from max_len if provided\n",
      "        model_max_length = kwargs.pop(\"model_max_length\", kwargs.pop(\"max_len\", None))\n",
      "        self.model_max_length = model_max_length if model_max_length is not None else VERY_LARGE_INTEGER\n",
      "\n",
      "        # Padding side is right by default and overridden in subclasses. If specified in the kwargs, it is changed.\n",
      "        self.padding_side = kwargs.pop(\"padding_side\", self.padding_side)\n",
      "        assert self.padding_side in [\n",
      "            \"right\",\n",
      "            \"left\",\n",
      "        ], f\"Padding side should be selected between 'right' and 'left', current value: {self.padding_side}\"\n",
      "        self.model_input_names = kwargs.pop(\"model_input_names\", self.model_input_names)\n",
      "\n",
      "        self.deprecation_warnings = (\n",
      "            {}\n",
      "        )  # Use to store when we have already noticed a deprecation warning (avoid overlogging).\n",
      "\n",
      "        super().__init__(**kwargs)\n",
      "\n",
      "    @property\n",
      "    def max_len_single_sentence(self) -> int:\n",
      "        \"\"\"\n",
      "        :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      "        \"\"\"\n",
      "        return self.model_max_length - self.num_special_tokens_to_add(pair=False)\n",
      "\n",
      "    @property\n",
      "    def max_len_sentences_pair(self) -> int:\n",
      "        \"\"\"\n",
      "        :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      "        \"\"\"\n",
      "        return self.model_max_length - self.num_special_tokens_to_add(pair=True)\n",
      "\n",
      "    @max_len_single_sentence.setter\n",
      "    def max_len_single_sentence(self, value) -> int:\n",
      "        # For backward compatibility, allow to try to setup 'max_len_single_sentence'.\n",
      "        if value == self.model_max_length - self.num_special_tokens_to_add(pair=False) and self.verbose:\n",
      "            if not self.deprecation_warnings.get(\"max_len_single_sentence\", False):\n",
      "                logger.warning(\n",
      "                    \"Setting 'max_len_single_sentence' is now deprecated. \" \"This value is automatically set up.\"\n",
      "                )\n",
      "            self.deprecation_warnings[\"max_len_single_sentence\"] = True\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Setting 'max_len_single_sentence' is now deprecated. \" \"This value is automatically set up.\"\n",
      "            )\n",
      "\n",
      "    @max_len_sentences_pair.setter\n",
      "    def max_len_sentences_pair(self, value) -> int:\n",
      "        # For backward compatibility, allow to try to setup 'max_len_sentences_pair'.\n",
      "        if value == self.model_max_length - self.num_special_tokens_to_add(pair=True) and self.verbose:\n",
      "            if not self.deprecation_warnings.get(\"max_len_sentences_pair\", False):\n",
      "                logger.warning(\n",
      "                    \"Setting 'max_len_sentences_pair' is now deprecated. \" \"This value is automatically set up.\"\n",
      "                )\n",
      "            self.deprecation_warnings[\"max_len_sentences_pair\"] = True\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Setting 'max_len_sentences_pair' is now deprecated. \" \"This value is automatically set up.\"\n",
      "            )\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        return (\n",
      "            f\"{'PreTrainedTokenizerFast' if self.is_fast else 'PreTrainedTokenizer'}(name_or_path='{self.name_or_path}', \"\n",
      "            f\"vocab_size={self.vocab_size}, model_max_len={self.model_max_length}, is_fast={self.is_fast}, \"\n",
      "            f\"padding_side='{self.padding_side}', special_tokens={self.special_tokens_map_extended})\"\n",
      "        )\n",
      "\n",
      "    def get_vocab(self) -> Dict[str, int]:\n",
      "        \"\"\"\n",
      "        Returns the vocabulary as a dictionary of token to index.\n",
      "\n",
      "        :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      "        :obj:`token` is in the vocab.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`Dict[str, int]`: The vocabulary.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    @classmethod\n",
      "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs):\n",
      "        r\"\"\"\n",
      "        Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      "        a predefined tokenizer.\n",
      "\n",
      "        Args:\n",
      "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
      "                Can be either:\n",
      "\n",
      "                - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      "                  Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
      "                  user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      "                - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      "                  using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      "                  method, e.g., ``./my_model_directory/``.\n",
      "                - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      "                  file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      "                  ``./my_model_directory/vocab.txt``.\n",
      "            cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      "                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      "                standard cache should not be used.\n",
      "            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      "                exist.\n",
      "            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      "                exists.\n",
      "            proxies (:obj:`Dict[str, str], `optional`):\n",
      "                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "            use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      "                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      "                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      "                identifier allowed by git.\n",
      "            subfolder (:obj:`str`, `optional`):\n",
      "                In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      "                facebook/rag-token-base), specify it here.\n",
      "            inputs (additional positional arguments, `optional`):\n",
      "                Will be passed along to the Tokenizer ``__init__`` method.\n",
      "            kwargs (additional keyword arguments, `optional`):\n",
      "                Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      "                ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      "                ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "            Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      "\n",
      "        Examples::\n",
      "\n",
      "            # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      "            # Download vocabulary from huggingface.co and cache.\n",
      "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "\n",
      "            # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      "            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      "\n",
      "            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      "\n",
      "            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      "\n",
      "            # You can link tokens to special vocabulary when instantiating\n",
      "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      "            # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      "            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      "            assert tokenizer.unk_token == '<unk>'\n",
      "\n",
      "        \"\"\"\n",
      "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
      "        force_download = kwargs.pop(\"force_download\", False)\n",
      "        resume_download = kwargs.pop(\"resume_download\", False)\n",
      "        proxies = kwargs.pop(\"proxies\", None)\n",
      "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
      "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
      "        revision = kwargs.pop(\"revision\", None)\n",
      "        subfolder = kwargs.pop(\"subfolder\", None)\n",
      "\n",
      "        if is_offline_mode() and not local_files_only:\n",
      "            logger.info(\"Offline mode: forcing local_files_only=True\")\n",
      "            local_files_only = True\n",
      "\n",
      "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
      "        vocab_files = {}\n",
      "        init_configuration = {}\n",
      "\n",
      "        if os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
      "            if len(cls.vocab_files_names) > 1:\n",
      "                raise ValueError(\n",
      "                    f\"Calling {cls.__name__}.from_pretrained() with the path to a single file or url is not \"\n",
      "                    \"supported for this tokenizer. Use a model identifier or the path to a directory instead.\"\n",
      "                )\n",
      "            warnings.warn(\n",
      "                f\"Calling {cls.__name__}.from_pretrained() with the path to a single file or url is deprecated and \"\n",
      "                \"won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\",\n",
      "                FutureWarning,\n",
      "            )\n",
      "            file_id = list(cls.vocab_files_names.keys())[0]\n",
      "            vocab_files[file_id] = pretrained_model_name_or_path\n",
      "        else:\n",
      "            # At this point pretrained_model_name_or_path is either a directory or a model identifier name\n",
      "            additional_files_names = {\n",
      "                \"added_tokens_file\": ADDED_TOKENS_FILE,\n",
      "                \"special_tokens_map_file\": SPECIAL_TOKENS_MAP_FILE,\n",
      "                \"tokenizer_config_file\": TOKENIZER_CONFIG_FILE,\n",
      "                \"tokenizer_file\": FULL_TOKENIZER_FILE,\n",
      "            }\n",
      "            # Look for the tokenizer files\n",
      "            for file_id, file_name in {**cls.vocab_files_names, **additional_files_names}.items():\n",
      "                if os.path.isdir(pretrained_model_name_or_path):\n",
      "                    if subfolder is not None:\n",
      "                        full_file_name = os.path.join(pretrained_model_name_or_path, subfolder, file_name)\n",
      "                    else:\n",
      "                        full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n",
      "                    if not os.path.exists(full_file_name):\n",
      "                        logger.info(f\"Didn't find file {full_file_name}. We won't load it.\")\n",
      "                        full_file_name = None\n",
      "                else:\n",
      "                    full_file_name = hf_bucket_url(\n",
      "                        pretrained_model_name_or_path,\n",
      "                        filename=file_name,\n",
      "                        subfolder=subfolder,\n",
      "                        revision=revision,\n",
      "                        mirror=None,\n",
      "                    )\n",
      "\n",
      "                vocab_files[file_id] = full_file_name\n",
      "\n",
      "        # Get files from url, cache, or disk depending on the case\n",
      "        resolved_vocab_files = {}\n",
      "        unresolved_files = []\n",
      "        for file_id, file_path in vocab_files.items():\n",
      "            if file_path is None:\n",
      "                resolved_vocab_files[file_id] = None\n",
      "            else:\n",
      "                try:\n",
      "                    resolved_vocab_files[file_id] = cached_path(\n",
      "                        file_path,\n",
      "                        cache_dir=cache_dir,\n",
      "                        force_download=force_download,\n",
      "                        proxies=proxies,\n",
      "                        resume_download=resume_download,\n",
      "                        local_files_only=local_files_only,\n",
      "                        use_auth_token=use_auth_token,\n",
      "                    )\n",
      "\n",
      "                except FileNotFoundError as error:\n",
      "                    if local_files_only:\n",
      "                        unresolved_files.append(file_id)\n",
      "                    else:\n",
      "                        raise error\n",
      "\n",
      "                except requests.exceptions.HTTPError as err:\n",
      "                    if \"404 Client Error\" in str(err):\n",
      "                        logger.debug(err)\n",
      "                        resolved_vocab_files[file_id] = None\n",
      "                    else:\n",
      "                        raise err\n",
      "\n",
      "        if len(unresolved_files) > 0:\n",
      "            logger.info(\n",
      "                f\"Can't load following files from cache: {unresolved_files} and cannot check if these \"\n",
      "                \"files are necessary for the tokenizer to operate.\"\n",
      "            )\n",
      "\n",
      "        if all(full_file_name is None for full_file_name in resolved_vocab_files.values()):\n",
      "            msg = (\n",
      "                f\"Can't load tokenizer for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
      "                f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
      "                f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\n",
      "            )\n",
      "            raise EnvironmentError(msg)\n",
      "\n",
      "        for file_id, file_path in vocab_files.items():\n",
      "            if file_id not in resolved_vocab_files:\n",
      "                continue\n",
      "\n",
      "            if file_path == resolved_vocab_files[file_id]:\n",
      "                logger.info(f\"loading file {file_path}\")\n",
      "            else:\n",
      "                logger.info(f\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\")\n",
      "\n",
      "        return cls._from_pretrained(\n",
      "            resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs\n",
      "        )\n",
      "\n",
      "    @classmethod\n",
      "    def _from_pretrained(\n",
      "        cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs\n",
      "    ):\n",
      "        # We instantiate fast tokenizers based on a slow tokenizer if we don't have access to the tokenizer.json\n",
      "        # file or if `from_slow` is set to True.\n",
      "        from_slow = kwargs.get(\"from_slow\", False)\n",
      "        has_tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None) is not None\n",
      "        if (from_slow or not has_tokenizer_file) and cls.slow_tokenizer_class is not None:\n",
      "            slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n",
      "                copy.deepcopy(resolved_vocab_files),\n",
      "                pretrained_model_name_or_path,\n",
      "                copy.deepcopy(init_configuration),\n",
      "                *init_inputs,\n",
      "                **(copy.deepcopy(kwargs)),\n",
      "            )\n",
      "        else:\n",
      "            slow_tokenizer = None\n",
      "\n",
      "        # Prepare tokenizer initialization kwargs\n",
      "        # Did we saved some inputs and kwargs to reload ?\n",
      "        tokenizer_config_file = resolved_vocab_files.pop(\"tokenizer_config_file\", None)\n",
      "        if tokenizer_config_file is not None:\n",
      "            with open(tokenizer_config_file, encoding=\"utf-8\") as tokenizer_config_handle:\n",
      "                init_kwargs = json.load(tokenizer_config_handle)\n",
      "            saved_init_inputs = init_kwargs.pop(\"init_inputs\", ())\n",
      "            if not init_inputs:\n",
      "                init_inputs = saved_init_inputs\n",
      "        else:\n",
      "            init_kwargs = init_configuration\n",
      "\n",
      "        # Update with newly provided kwargs\n",
      "        init_kwargs.update(kwargs)\n",
      "\n",
      "        # Convert AddedTokens serialized as dict to class instances\n",
      "        def convert_added_tokens(obj: Union[AddedToken, Any]):\n",
      "            if isinstance(obj, dict) and \"__type\" in obj and obj[\"__type\"] == \"AddedToken\":\n",
      "                obj.pop(\"__type\")\n",
      "                return AddedToken(**obj)\n",
      "            elif isinstance(obj, (list, tuple)):\n",
      "                return list(convert_added_tokens(o) for o in obj)\n",
      "            elif isinstance(obj, dict):\n",
      "                return {k: convert_added_tokens(v) for k, v in obj.items()}\n",
      "            return obj\n",
      "\n",
      "        init_kwargs = convert_added_tokens(init_kwargs)\n",
      "\n",
      "        # Set max length if needed\n",
      "        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n",
      "            # if we're using a pretrained model, ensure the tokenizer\n",
      "            # wont index sequences longer than the number of positional embeddings\n",
      "            model_max_length = cls.max_model_input_sizes[pretrained_model_name_or_path]\n",
      "            if model_max_length is not None and isinstance(model_max_length, (int, float)):\n",
      "                init_kwargs[\"model_max_length\"] = min(init_kwargs.get(\"model_max_length\", int(1e30)), model_max_length)\n",
      "\n",
      "        # Merge resolved_vocab_files arguments in init_kwargs.\n",
      "        added_tokens_file = resolved_vocab_files.pop(\"added_tokens_file\", None)\n",
      "        for args_name, file_path in resolved_vocab_files.items():\n",
      "            if args_name not in init_kwargs:\n",
      "                init_kwargs[args_name] = file_path\n",
      "\n",
      "        if slow_tokenizer is not None:\n",
      "            init_kwargs[\"__slow_tokenizer\"] = slow_tokenizer\n",
      "\n",
      "        init_kwargs[\"name_or_path\"] = pretrained_model_name_or_path\n",
      "\n",
      "        # Instantiate tokenizer.\n",
      "        try:\n",
      "            tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "        except OSError:\n",
      "            raise OSError(\n",
      "                \"Unable to load vocabulary from file. \"\n",
      "                \"Please check that the provided vocabulary is accessible and not corrupted.\"\n",
      "            )\n",
      "\n",
      "        # Save inputs and kwargs for saving and re-loading with ``save_pretrained``\n",
      "        # Removed: Now done at the base class level\n",
      "        # tokenizer.init_inputs = init_inputs\n",
      "        # tokenizer.init_kwargs = init_kwargs\n",
      "\n",
      "        # If there is a complementary special token map, load it\n",
      "        special_tokens_map_file = resolved_vocab_files.pop(\"special_tokens_map_file\", None)\n",
      "        if special_tokens_map_file is not None:\n",
      "            with open(special_tokens_map_file, encoding=\"utf-8\") as special_tokens_map_handle:\n",
      "                special_tokens_map = json.load(special_tokens_map_handle)\n",
      "            for key, value in special_tokens_map.items():\n",
      "                if isinstance(value, dict):\n",
      "                    value = AddedToken(**value)\n",
      "                elif isinstance(value, list):\n",
      "                    value = [AddedToken(**token) if isinstance(token, dict) else token for token in value]\n",
      "                setattr(tokenizer, key, value)\n",
      "\n",
      "        # Add supplementary tokens.\n",
      "        special_tokens = tokenizer.all_special_tokens\n",
      "        if added_tokens_file is not None:\n",
      "            with open(added_tokens_file, encoding=\"utf-8\") as added_tokens_handle:\n",
      "                added_tok_encoder = json.load(added_tokens_handle)\n",
      "\n",
      "            # Sort added tokens by index\n",
      "            added_tok_encoder_sorted = list(sorted(added_tok_encoder.items(), key=lambda x: x[1]))\n",
      "\n",
      "            for token, index in added_tok_encoder_sorted:\n",
      "                assert index == len(tokenizer), (\n",
      "                    f\"Non-consecutive added token '{token}' found. \"\n",
      "                    f\"Should have index {len(tokenizer)} but has index {index} in saved vocabulary.\"\n",
      "                )\n",
      "                tokenizer.add_tokens(token, special_tokens=bool(token in special_tokens))\n",
      "\n",
      "        # Check all our special tokens are registered as \"no split\" token (we don't cut them) and are in the vocab\n",
      "        added_tokens = tokenizer.sanitize_special_tokens()\n",
      "        if added_tokens:\n",
      "            logger.warning(\n",
      "                \"Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\"\n",
      "            )\n",
      "\n",
      "        return tokenizer\n",
      "\n",
      "    def save_pretrained(\n",
      "        self,\n",
      "        save_directory: Union[str, os.PathLike],\n",
      "        legacy_format: bool = True,\n",
      "        filename_prefix: Optional[str] = None,\n",
      "    ) -> Tuple[str]:\n",
      "        \"\"\"\n",
      "        Save the full tokenizer state.\n",
      "\n",
      "\n",
      "        This method make sure the full tokenizer can then be re-loaded using the\n",
      "        :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method.\n",
      "\n",
      "        .. Note::\n",
      "            A \"fast\" tokenizer (instance of :class:`transformers.PreTrainedTokenizerFast`) saved with this method will\n",
      "            not be possible to load back in a \"slow\" tokenizer, i.e. in a :class:`transformers.PreTrainedTokenizer`\n",
      "            instance. It can only be loaded in a \"fast\" tokenizer, i.e. in a\n",
      "            :class:`transformers.PreTrainedTokenizerFast` instance.\n",
      "\n",
      "        .. Warning::\n",
      "           This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      "           modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      "\n",
      "        Args:\n",
      "            save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      "            legacy_format (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "                Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a\n",
      "                separate added_tokens files or in the unified JSON file format for the `tokenizers` library. It's only\n",
      "                possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with\n",
      "                \"slow\" tokenizers (not powered by the `tokenizers` library).\n",
      "            filename_prefix: (:obj:`str`, `optional`):\n",
      "                A prefix to add to the names of the files saved by the tokenizer.\n",
      "\n",
      "        Returns:\n",
      "            A tuple of :obj:`str`: The files saved.\n",
      "        \"\"\"\n",
      "        if os.path.isfile(save_directory):\n",
      "            logger.error(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n",
      "            return\n",
      "        os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "        special_tokens_map_file = os.path.join(\n",
      "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + SPECIAL_TOKENS_MAP_FILE\n",
      "        )\n",
      "        tokenizer_config_file = os.path.join(\n",
      "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + TOKENIZER_CONFIG_FILE\n",
      "        )\n",
      "\n",
      "        tokenizer_config = copy.deepcopy(self.init_kwargs)\n",
      "        if len(self.init_inputs) > 0:\n",
      "            tokenizer_config[\"init_inputs\"] = copy.deepcopy(self.init_inputs)\n",
      "        for file_id in self.vocab_files_names.keys():\n",
      "            tokenizer_config.pop(file_id, None)\n",
      "\n",
      "        # Sanitize AddedTokens\n",
      "        def convert_added_tokens(obj: Union[AddedToken, Any], add_type_field=True):\n",
      "            if isinstance(obj, AddedToken):\n",
      "                out = obj.__getstate__()\n",
      "                if add_type_field:\n",
      "                    out[\"__type\"] = \"AddedToken\"\n",
      "                return out\n",
      "            elif isinstance(obj, (list, tuple)):\n",
      "                return list(convert_added_tokens(o, add_type_field=add_type_field) for o in obj)\n",
      "            elif isinstance(obj, dict):\n",
      "                return {k: convert_added_tokens(v, add_type_field=add_type_field) for k, v in obj.items()}\n",
      "            return obj\n",
      "\n",
      "        # add_type_field=True to allow dicts in the kwargs / differentiate from AddedToken serialization\n",
      "        tokenizer_config = convert_added_tokens(tokenizer_config, add_type_field=True)\n",
      "        with open(tokenizer_config_file, \"w\", encoding=\"utf-8\") as f:\n",
      "            f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n",
      "        logger.info(f\"tokenizer config file saved in {tokenizer_config_file}\")\n",
      "\n",
      "        # Sanitize AddedTokens in special_tokens_map\n",
      "        write_dict = convert_added_tokens(self.special_tokens_map_extended, add_type_field=False)\n",
      "        with open(special_tokens_map_file, \"w\", encoding=\"utf-8\") as f:\n",
      "            f.write(json.dumps(write_dict, ensure_ascii=False))\n",
      "        logger.info(f\"Special tokens file saved in {special_tokens_map_file}\")\n",
      "\n",
      "        file_names = (tokenizer_config_file, special_tokens_map_file)\n",
      "\n",
      "        return self._save_pretrained(\n",
      "            save_directory=save_directory,\n",
      "            file_names=file_names,\n",
      "            legacy_format=legacy_format,\n",
      "            filename_prefix=filename_prefix,\n",
      "        )\n",
      "\n",
      "    def _save_pretrained(\n",
      "        self,\n",
      "        save_directory: Union[str, os.PathLike],\n",
      "        file_names: Tuple[str],\n",
      "        legacy_format: bool = True,\n",
      "        filename_prefix: Optional[str] = None,\n",
      "    ) -> Tuple[str]:\n",
      "        \"\"\"\n",
      "        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens.\n",
      "\n",
      "        Fast tokenizers can also be saved in a unique JSON file containing {config + vocab + added-tokens} using the\n",
      "        specific :meth:`~transformers.tokenization_utils_fast.PreTrainedTokenizerFast._save_pretrained`\n",
      "        \"\"\"\n",
      "        if not legacy_format:\n",
      "            raise ValueError(\n",
      "                \"Only fast tokenizers (instances of PretrainedTokenizerFast) can be saved in non legacy format.\"\n",
      "            )\n",
      "\n",
      "        save_directory = str(save_directory)\n",
      "\n",
      "        added_tokens_file = os.path.join(\n",
      "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + ADDED_TOKENS_FILE\n",
      "        )\n",
      "        added_vocab = self.get_added_vocab()\n",
      "        if added_vocab:\n",
      "            with open(added_tokens_file, \"w\", encoding=\"utf-8\") as f:\n",
      "                out_str = json.dumps(added_vocab, ensure_ascii=False)\n",
      "                f.write(out_str)\n",
      "                logger.info(f\"added tokens file saved in {added_tokens_file}\")\n",
      "\n",
      "        vocab_files = self.save_vocabulary(save_directory, filename_prefix=filename_prefix)\n",
      "\n",
      "        return file_names + vocab_files + (added_tokens_file,)\n",
      "\n",
      "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
      "        \"\"\"\n",
      "        Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      "\n",
      "        This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      "        :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.\n",
      "\n",
      "        Args:\n",
      "            save_directory (:obj:`str`):\n",
      "                The directory in which to save the vocabulary.\n",
      "            filename_prefix (:obj:`str`, `optional`):\n",
      "                An optional prefix to add to the named of the saved files.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`Tuple(str)`: Paths to the files saved.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]:\n",
      "        \"\"\"\n",
      "        Converts a string in a sequence of tokens, replacing unknown tokens with the :obj:`unk_token`.\n",
      "\n",
      "        Args:\n",
      "            text (:obj:`str`):\n",
      "                The sequence to be encoded.\n",
      "            pair (:obj:`str`, `optional`):\n",
      "                A second sequence to be encoded with the first.\n",
      "            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to add the special tokens associated with the corresponding model.\n",
      "            kwargs (additional keyword arguments, `optional`):\n",
      "                Will be passed to the underlying model specific encode method. See details in\n",
      "                :meth:`~transformers.PreTrainedTokenizerBase.__call__`\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[str]`: The list of tokens.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @add_end_docstrings(\n",
      "        ENCODE_KWARGS_DOCSTRING,\n",
      "        \"\"\"\n",
      "            **kwargs: Passed along to the `.tokenize()` method.\n",
      "        \"\"\",\n",
      "        \"\"\"\n",
      "        Returns:\n",
      "            :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the\n",
      "            text.\n",
      "        \"\"\",\n",
      "    )\n",
      "    def encode(\n",
      "        self,\n",
      "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
      "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
      "        add_special_tokens: bool = True,\n",
      "        padding: Union[bool, str, PaddingStrategy] = False,\n",
      "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        **kwargs\n",
      "    ) -> List[int]:\n",
      "        \"\"\"\n",
      "        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      "\n",
      "        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      "\n",
      "        Args:\n",
      "            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      "                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      "                method).\n",
      "            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      "                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      "                ``convert_tokens_to_ids`` method).\n",
      "        \"\"\"\n",
      "        encoded_inputs = self.encode_plus(\n",
      "            text,\n",
      "            text_pair=text_pair,\n",
      "            add_special_tokens=add_special_tokens,\n",
      "            padding=padding,\n",
      "            truncation=truncation,\n",
      "            max_length=max_length,\n",
      "            stride=stride,\n",
      "            return_tensors=return_tensors,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        return encoded_inputs[\"input_ids\"]\n",
      "\n",
      "    def num_special_tokens_to_add(self, pair: bool = False) -> int:\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def _get_padding_truncation_strategies(\n",
      "        self, padding=False, truncation=False, max_length=None, pad_to_multiple_of=None, verbose=True, **kwargs\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Find the correct padding/truncation strategy with backward compatibility for old arguments (truncation_strategy\n",
      "        and pad_to_max_length) and behaviors.\n",
      "        \"\"\"\n",
      "        old_truncation_strategy = kwargs.pop(\"truncation_strategy\", \"do_not_truncate\")\n",
      "        old_pad_to_max_length = kwargs.pop(\"pad_to_max_length\", False)\n",
      "\n",
      "        # Backward compatibility for previous behavior, maybe we should deprecate it:\n",
      "        # If you only set max_length, it activates truncation for max_length\n",
      "        if max_length is not None and padding is False and truncation is False:\n",
      "            if verbose:\n",
      "                if not self.deprecation_warnings.get(\"Truncation-not-explicitly-activated\", False):\n",
      "                    logger.warning(\n",
      "                        \"Truncation was not explicitly activated but `max_length` is provided a specific value, \"\n",
      "                        \"please use `truncation=True` to explicitly truncate examples to max length. \"\n",
      "                        \"Defaulting to 'longest_first' truncation strategy. \"\n",
      "                        \"If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy \"\n",
      "                        \"more precisely by providing a specific strategy to `truncation`.\"\n",
      "                    )\n",
      "                self.deprecation_warnings[\"Truncation-not-explicitly-activated\"] = True\n",
      "            truncation = \"longest_first\"\n",
      "\n",
      "        # Get padding strategy\n",
      "        if padding is False and old_pad_to_max_length:\n",
      "            if verbose:\n",
      "                warnings.warn(\n",
      "                    \"The `pad_to_max_length` argument is deprecated and will be removed in a future version, \"\n",
      "                    \"use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or \"\n",
      "                    \"use `padding='max_length'` to pad to a max length. In this case, you can give a specific \"\n",
      "                    \"length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the \"\n",
      "                    \"maximal input size of the model (e.g. 512 for Bert).\",\n",
      "                    FutureWarning,\n",
      "                )\n",
      "            if max_length is None:\n",
      "                padding_strategy = PaddingStrategy.LONGEST\n",
      "            else:\n",
      "                padding_strategy = PaddingStrategy.MAX_LENGTH\n",
      "        elif padding is not False:\n",
      "            if padding is True:\n",
      "                padding_strategy = PaddingStrategy.LONGEST  # Default to pad to the longest sequence in the batch\n",
      "            elif not isinstance(padding, PaddingStrategy):\n",
      "                padding_strategy = PaddingStrategy(padding)\n",
      "            elif isinstance(padding, PaddingStrategy):\n",
      "                padding_strategy = padding\n",
      "        else:\n",
      "            padding_strategy = PaddingStrategy.DO_NOT_PAD\n",
      "\n",
      "        # Get truncation strategy\n",
      "        if truncation is False and old_truncation_strategy != \"do_not_truncate\":\n",
      "            if verbose:\n",
      "                warnings.warn(\n",
      "                    \"The `truncation_strategy` argument is deprecated and will be removed in a future version, \"\n",
      "                    \"use `truncation=True` to truncate examples to a max length. You can give a specific \"\n",
      "                    \"length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the \"\n",
      "                    \"maximal input size of the model (e.g. 512 for Bert). \"\n",
      "                    \" If you have pairs of inputs, you can give a specific truncation strategy selected among \"\n",
      "                    \"`truncation='only_first'` (will only truncate the first sentence in the pairs) \"\n",
      "                    \"`truncation='only_second'` (will only truncate the second sentence in the pairs) \"\n",
      "                    \"or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\",\n",
      "                    FutureWarning,\n",
      "                )\n",
      "            truncation_strategy = TruncationStrategy(old_truncation_strategy)\n",
      "        elif truncation is not False:\n",
      "            if truncation is True:\n",
      "                truncation_strategy = (\n",
      "                    TruncationStrategy.LONGEST_FIRST\n",
      "                )  # Default to truncate the longest sequences in pairs of inputs\n",
      "            elif not isinstance(truncation, TruncationStrategy):\n",
      "                truncation_strategy = TruncationStrategy(truncation)\n",
      "            elif isinstance(truncation, TruncationStrategy):\n",
      "                truncation_strategy = truncation\n",
      "        else:\n",
      "            truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE\n",
      "\n",
      "        # Set max length if needed\n",
      "        if max_length is None:\n",
      "            if padding_strategy == PaddingStrategy.MAX_LENGTH:\n",
      "                if self.model_max_length > LARGE_INTEGER:\n",
      "                    if verbose:\n",
      "                        if not self.deprecation_warnings.get(\"Asking-to-pad-to-max_length\", False):\n",
      "                            logger.warning(\n",
      "                                \"Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. \"\n",
      "                                \"Default to no padding.\"\n",
      "                            )\n",
      "                        self.deprecation_warnings[\"Asking-to-pad-to-max_length\"] = True\n",
      "                    padding_strategy = PaddingStrategy.DO_NOT_PAD\n",
      "                else:\n",
      "                    max_length = self.model_max_length\n",
      "\n",
      "            if truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE:\n",
      "                if self.model_max_length > LARGE_INTEGER:\n",
      "                    if verbose:\n",
      "                        if not self.deprecation_warnings.get(\"Asking-to-truncate-to-max_length\", False):\n",
      "                            logger.warning(\n",
      "                                \"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. \"\n",
      "                                \"Default to no truncation.\"\n",
      "                            )\n",
      "                        self.deprecation_warnings[\"Asking-to-truncate-to-max_length\"] = True\n",
      "                    truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE\n",
      "                else:\n",
      "                    max_length = self.model_max_length\n",
      "\n",
      "        # Test if we have a padding token\n",
      "        if padding_strategy != PaddingStrategy.DO_NOT_PAD and (not self.pad_token or self.pad_token_id < 0):\n",
      "            raise ValueError(\n",
      "                \"Asking to pad but the tokenizer does not have a padding token. \"\n",
      "                \"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\n",
      "                \"or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\"\n",
      "            )\n",
      "\n",
      "        # Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\n",
      "        if (\n",
      "            truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE\n",
      "            and padding_strategy != PaddingStrategy.DO_NOT_PAD\n",
      "            and pad_to_multiple_of is not None\n",
      "            and max_length is not None\n",
      "            and (max_length % pad_to_multiple_of != 0)\n",
      "        ):\n",
      "            raise ValueError(\n",
      "                f\"Truncation and padding are both activated but \"\n",
      "                f\"truncation length ({max_length}) is not a multiple of pad_to_multiple_of ({pad_to_multiple_of}).\"\n",
      "            )\n",
      "\n",
      "        return padding_strategy, truncation_strategy, max_length, kwargs\n",
      "\n",
      "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
      "    def __call__(\n",
      "        self,\n",
      "        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n",
      "        text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n",
      "        add_special_tokens: bool = True,\n",
      "        padding: Union[bool, str, PaddingStrategy] = False,\n",
      "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        is_split_into_words: bool = False,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        return_token_type_ids: Optional[bool] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_overflowing_tokens: bool = False,\n",
      "        return_special_tokens_mask: bool = False,\n",
      "        return_offsets_mapping: bool = False,\n",
      "        return_length: bool = False,\n",
      "        verbose: bool = True,\n",
      "        **kwargs\n",
      "    ) -> BatchEncoding:\n",
      "        \"\"\"\n",
      "        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "        sequences.\n",
      "\n",
      "        Args:\n",
      "            text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "                :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "            text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "                :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        \"\"\"\n",
      "        # Input type checking for clearer error\n",
      "        assert isinstance(text, str) or (\n",
      "            isinstance(text, (list, tuple))\n",
      "            and (\n",
      "                len(text) == 0\n",
      "                or (\n",
      "                    isinstance(text[0], str)\n",
      "                    or (isinstance(text[0], (list, tuple)) and (len(text[0]) == 0 or isinstance(text[0][0], str)))\n",
      "                )\n",
      "            )\n",
      "        ), (\n",
      "            \"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\n",
      "            \"or `List[List[str]]` (batch of pretokenized examples).\"\n",
      "        )\n",
      "\n",
      "        assert (\n",
      "            text_pair is None\n",
      "            or isinstance(text_pair, str)\n",
      "            or (\n",
      "                isinstance(text_pair, (list, tuple))\n",
      "                and (\n",
      "                    len(text_pair) == 0\n",
      "                    or (\n",
      "                        isinstance(text_pair[0], str)\n",
      "                        or (\n",
      "                            isinstance(text_pair[0], (list, tuple))\n",
      "                            and (len(text_pair[0]) == 0 or isinstance(text_pair[0][0], str))\n",
      "                        )\n",
      "                    )\n",
      "                )\n",
      "            )\n",
      "        ), (\n",
      "            \"text_pair input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\n",
      "            \"or `List[List[str]]` (batch of pretokenized examples).\"\n",
      "        )\n",
      "\n",
      "        is_batched = bool(\n",
      "            (not is_split_into_words and isinstance(text, (list, tuple)))\n",
      "            or (\n",
      "                is_split_into_words and isinstance(text, (list, tuple)) and text and isinstance(text[0], (list, tuple))\n",
      "            )\n",
      "        )\n",
      "\n",
      "        if is_batched:\n",
      "            batch_text_or_text_pairs = list(zip(text, text_pair)) if text_pair is not None else text\n",
      "            return self.batch_encode_plus(\n",
      "                batch_text_or_text_pairs=batch_text_or_text_pairs,\n",
      "                add_special_tokens=add_special_tokens,\n",
      "                padding=padding,\n",
      "                truncation=truncation,\n",
      "                max_length=max_length,\n",
      "                stride=stride,\n",
      "                is_split_into_words=is_split_into_words,\n",
      "                pad_to_multiple_of=pad_to_multiple_of,\n",
      "                return_tensors=return_tensors,\n",
      "                return_token_type_ids=return_token_type_ids,\n",
      "                return_attention_mask=return_attention_mask,\n",
      "                return_overflowing_tokens=return_overflowing_tokens,\n",
      "                return_special_tokens_mask=return_special_tokens_mask,\n",
      "                return_offsets_mapping=return_offsets_mapping,\n",
      "                return_length=return_length,\n",
      "                verbose=verbose,\n",
      "                **kwargs,\n",
      "            )\n",
      "        else:\n",
      "            return self.encode_plus(\n",
      "                text=text,\n",
      "                text_pair=text_pair,\n",
      "                add_special_tokens=add_special_tokens,\n",
      "                padding=padding,\n",
      "                truncation=truncation,\n",
      "                max_length=max_length,\n",
      "                stride=stride,\n",
      "                is_split_into_words=is_split_into_words,\n",
      "                pad_to_multiple_of=pad_to_multiple_of,\n",
      "                return_tensors=return_tensors,\n",
      "                return_token_type_ids=return_token_type_ids,\n",
      "                return_attention_mask=return_attention_mask,\n",
      "                return_overflowing_tokens=return_overflowing_tokens,\n",
      "                return_special_tokens_mask=return_special_tokens_mask,\n",
      "                return_offsets_mapping=return_offsets_mapping,\n",
      "                return_length=return_length,\n",
      "                verbose=verbose,\n",
      "                **kwargs,\n",
      "            )\n",
      "\n",
      "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
      "    def encode_plus(\n",
      "        self,\n",
      "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
      "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
      "        add_special_tokens: bool = True,\n",
      "        padding: Union[bool, str, PaddingStrategy] = False,\n",
      "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        is_split_into_words: bool = False,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        return_token_type_ids: Optional[bool] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_overflowing_tokens: bool = False,\n",
      "        return_special_tokens_mask: bool = False,\n",
      "        return_offsets_mapping: bool = False,\n",
      "        return_length: bool = False,\n",
      "        verbose: bool = True,\n",
      "        **kwargs\n",
      "    ) -> BatchEncoding:\n",
      "        \"\"\"\n",
      "        Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      "\n",
      "        .. warning::\n",
      "            This method is deprecated, ``__call__`` should be used instead.\n",
      "\n",
      "        Args:\n",
      "            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      "                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      "                method).\n",
      "            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      "                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      "                ``convert_tokens_to_ids`` method).\n",
      "        \"\"\"\n",
      "\n",
      "        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n",
      "        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
      "            padding=padding,\n",
      "            truncation=truncation,\n",
      "            max_length=max_length,\n",
      "            pad_to_multiple_of=pad_to_multiple_of,\n",
      "            verbose=verbose,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        return self._encode_plus(\n",
      "            text=text,\n",
      "            text_pair=text_pair,\n",
      "            add_special_tokens=add_special_tokens,\n",
      "            padding_strategy=padding_strategy,\n",
      "            truncation_strategy=truncation_strategy,\n",
      "            max_length=max_length,\n",
      "            stride=stride,\n",
      "            is_split_into_words=is_split_into_words,\n",
      "            pad_to_multiple_of=pad_to_multiple_of,\n",
      "            return_tensors=return_tensors,\n",
      "            return_token_type_ids=return_token_type_ids,\n",
      "            return_attention_mask=return_attention_mask,\n",
      "            return_overflowing_tokens=return_overflowing_tokens,\n",
      "            return_special_tokens_mask=return_special_tokens_mask,\n",
      "            return_offsets_mapping=return_offsets_mapping,\n",
      "            return_length=return_length,\n",
      "            verbose=verbose,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "    def _encode_plus(\n",
      "        self,\n",
      "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
      "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
      "        add_special_tokens: bool = True,\n",
      "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
      "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        is_split_into_words: bool = False,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        return_token_type_ids: Optional[bool] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_overflowing_tokens: bool = False,\n",
      "        return_special_tokens_mask: bool = False,\n",
      "        return_offsets_mapping: bool = False,\n",
      "        return_length: bool = False,\n",
      "        verbose: bool = True,\n",
      "        **kwargs\n",
      "    ) -> BatchEncoding:\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
      "    def batch_encode_plus(\n",
      "        self,\n",
      "        batch_text_or_text_pairs: Union[\n",
      "            List[TextInput],\n",
      "            List[TextInputPair],\n",
      "            List[PreTokenizedInput],\n",
      "            List[PreTokenizedInputPair],\n",
      "            List[EncodedInput],\n",
      "            List[EncodedInputPair],\n",
      "        ],\n",
      "        add_special_tokens: bool = True,\n",
      "        padding: Union[bool, str, PaddingStrategy] = False,\n",
      "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        is_split_into_words: bool = False,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        return_token_type_ids: Optional[bool] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_overflowing_tokens: bool = False,\n",
      "        return_special_tokens_mask: bool = False,\n",
      "        return_offsets_mapping: bool = False,\n",
      "        return_length: bool = False,\n",
      "        verbose: bool = True,\n",
      "        **kwargs\n",
      "    ) -> BatchEncoding:\n",
      "        \"\"\"\n",
      "        Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      "\n",
      "        .. warning::\n",
      "            This method is deprecated, ``__call__`` should be used instead.\n",
      "\n",
      "        Args:\n",
      "            batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      "                Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      "                string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      "                details in ``encode_plus``).\n",
      "        \"\"\"\n",
      "\n",
      "        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n",
      "        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
      "            padding=padding,\n",
      "            truncation=truncation,\n",
      "            max_length=max_length,\n",
      "            pad_to_multiple_of=pad_to_multiple_of,\n",
      "            verbose=verbose,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        return self._batch_encode_plus(\n",
      "            batch_text_or_text_pairs=batch_text_or_text_pairs,\n",
      "            add_special_tokens=add_special_tokens,\n",
      "            padding_strategy=padding_strategy,\n",
      "            truncation_strategy=truncation_strategy,\n",
      "            max_length=max_length,\n",
      "            stride=stride,\n",
      "            is_split_into_words=is_split_into_words,\n",
      "            pad_to_multiple_of=pad_to_multiple_of,\n",
      "            return_tensors=return_tensors,\n",
      "            return_token_type_ids=return_token_type_ids,\n",
      "            return_attention_mask=return_attention_mask,\n",
      "            return_overflowing_tokens=return_overflowing_tokens,\n",
      "            return_special_tokens_mask=return_special_tokens_mask,\n",
      "            return_offsets_mapping=return_offsets_mapping,\n",
      "            return_length=return_length,\n",
      "            verbose=verbose,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "    def _batch_encode_plus(\n",
      "        self,\n",
      "        batch_text_or_text_pairs: Union[\n",
      "            List[TextInput],\n",
      "            List[TextInputPair],\n",
      "            List[PreTokenizedInput],\n",
      "            List[PreTokenizedInputPair],\n",
      "            List[EncodedInput],\n",
      "            List[EncodedInputPair],\n",
      "        ],\n",
      "        add_special_tokens: bool = True,\n",
      "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
      "        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        is_split_into_words: bool = False,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        return_token_type_ids: Optional[bool] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_overflowing_tokens: bool = False,\n",
      "        return_special_tokens_mask: bool = False,\n",
      "        return_offsets_mapping: bool = False,\n",
      "        return_length: bool = False,\n",
      "        verbose: bool = True,\n",
      "        **kwargs\n",
      "    ) -> BatchEncoding:\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def pad(\n",
      "        self,\n",
      "        encoded_inputs: Union[\n",
      "            BatchEncoding,\n",
      "            List[BatchEncoding],\n",
      "            Dict[str, EncodedInput],\n",
      "            Dict[str, List[EncodedInput]],\n",
      "            List[Dict[str, EncodedInput]],\n",
      "        ],\n",
      "        padding: Union[bool, str, PaddingStrategy] = True,\n",
      "        max_length: Optional[int] = None,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        verbose: bool = True,\n",
      "    ) -> BatchEncoding:\n",
      "        \"\"\"\n",
      "        Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      "        in the batch.\n",
      "\n",
      "        Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,\n",
      "        ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      "\n",
      "        .. note::\n",
      "\n",
      "            If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      "            result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      "            case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      "\n",
      "        Args:\n",
      "            encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      "                Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,\n",
      "                List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,\n",
      "                List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as\n",
      "                well as in a PyTorch Dataloader collate function.\n",
      "\n",
      "                Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      "                see the note above for the return type.\n",
      "            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
      "                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      "                 index) among:\n",
      "\n",
      "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      "                  single sequence if provided).\n",
      "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      "                  maximum acceptable input length for the model if that argument is not provided.\n",
      "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      "                  different lengths).\n",
      "            max_length (:obj:`int`, `optional`):\n",
      "                Maximum length of the returned list and optionally padding length (see above).\n",
      "            pad_to_multiple_of (:obj:`int`, `optional`):\n",
      "                If set will pad the sequence to a multiple of the provided value.\n",
      "\n",
      "                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "                >= 7.5 (Volta).\n",
      "            return_attention_mask (:obj:`bool`, `optional`):\n",
      "                Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      "\n",
      "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
      "            return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      "                If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "                * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      "                * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      "                * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      "            verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "                Whether or not to print more information and warnings.\n",
      "        \"\"\"\n",
      "        # If we have a list of dicts, let's convert it in a dict of lists\n",
      "        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n",
      "        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):\n",
      "            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n",
      "\n",
      "        # The model's main input name, usually `input_ids`, has be passed for padding\n",
      "        if self.model_input_names[0] not in encoded_inputs:\n",
      "            raise ValueError(\n",
      "                \"You should supply an encoding or a list of encodings to this method\"\n",
      "                f\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\n",
      "            )\n",
      "\n",
      "        required_input = encoded_inputs[self.model_input_names[0]]\n",
      "\n",
      "        if not required_input:\n",
      "            if return_attention_mask:\n",
      "                encoded_inputs[\"attention_mask\"] = []\n",
      "            return encoded_inputs\n",
      "\n",
      "        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects\n",
      "        # and rebuild them afterwards if no return_tensors is specified\n",
      "        # Note that we lose the specific device the tensor may be on for PyTorch\n",
      "\n",
      "        first_element = required_input[0]\n",
      "        if isinstance(first_element, (list, tuple)):\n",
      "            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.\n",
      "            index = 0\n",
      "            while len(required_input[index]) == 0:\n",
      "                index += 1\n",
      "            if index < len(required_input):\n",
      "                first_element = required_input[index][0]\n",
      "        # At this state, if `first_element` is still a list/tuple, it's an empty one so there is nothing to do.\n",
      "        if not isinstance(first_element, (int, list, tuple)):\n",
      "            if is_tf_available() and _is_tensorflow(first_element):\n",
      "                return_tensors = \"tf\" if return_tensors is None else return_tensors\n",
      "            elif is_torch_available() and _is_torch(first_element):\n",
      "                return_tensors = \"pt\" if return_tensors is None else return_tensors\n",
      "            elif isinstance(first_element, np.ndarray):\n",
      "                return_tensors = \"np\" if return_tensors is None else return_tensors\n",
      "            else:\n",
      "                raise ValueError(\n",
      "                    f\"type of {first_element} unknown: {type(first_element)}. \"\n",
      "                    f\"Should be one of a python, numpy, pytorch or tensorflow object.\"\n",
      "                )\n",
      "\n",
      "            for key, value in encoded_inputs.items():\n",
      "                encoded_inputs[key] = to_py_obj(value)\n",
      "\n",
      "        # Convert padding_strategy in PaddingStrategy\n",
      "        padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(\n",
      "            padding=padding, max_length=max_length, verbose=verbose\n",
      "        )\n",
      "\n",
      "        required_input = encoded_inputs[self.model_input_names[0]]\n",
      "        if required_input and not isinstance(required_input[0], (list, tuple)):\n",
      "            encoded_inputs = self._pad(\n",
      "                encoded_inputs,\n",
      "                max_length=max_length,\n",
      "                padding_strategy=padding_strategy,\n",
      "                pad_to_multiple_of=pad_to_multiple_of,\n",
      "                return_attention_mask=return_attention_mask,\n",
      "            )\n",
      "            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n",
      "\n",
      "        batch_size = len(required_input)\n",
      "        assert all(\n",
      "            len(v) == batch_size for v in encoded_inputs.values()\n",
      "        ), \"Some items in the output dictionary have a different batch size than others.\"\n",
      "\n",
      "        if padding_strategy == PaddingStrategy.LONGEST:\n",
      "            max_length = max(len(inputs) for inputs in required_input)\n",
      "            padding_strategy = PaddingStrategy.MAX_LENGTH\n",
      "\n",
      "        batch_outputs = {}\n",
      "        for i in range(batch_size):\n",
      "            inputs = dict((k, v[i]) for k, v in encoded_inputs.items())\n",
      "            outputs = self._pad(\n",
      "                inputs,\n",
      "                max_length=max_length,\n",
      "                padding_strategy=padding_strategy,\n",
      "                pad_to_multiple_of=pad_to_multiple_of,\n",
      "                return_attention_mask=return_attention_mask,\n",
      "            )\n",
      "\n",
      "            for key, value in outputs.items():\n",
      "                if key not in batch_outputs:\n",
      "                    batch_outputs[key] = []\n",
      "                batch_outputs[key].append(value)\n",
      "\n",
      "        return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "    def create_token_type_ids_from_sequences(\n",
      "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
      "    ) -> List[int]:\n",
      "        \"\"\"\n",
      "        Create the token type IDs corresponding to the sequences passed. `What are token type IDs?\n",
      "        <../glossary.html#token-type-ids>`__\n",
      "\n",
      "        Should be overridden in a subclass if the model has a special way of building those.\n",
      "\n",
      "        Args:\n",
      "            token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      "            token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[int]`: The token type ids.\n",
      "        \"\"\"\n",
      "        if token_ids_1 is None:\n",
      "            return len(token_ids_0) * [0]\n",
      "        return [0] * len(token_ids_0) + [1] * len(token_ids_1)\n",
      "\n",
      "    def build_inputs_with_special_tokens(\n",
      "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
      "    ) -> List[int]:\n",
      "        \"\"\"\n",
      "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      "        adding special tokens.\n",
      "\n",
      "        This implementation does not add special tokens and this method should be overridden in a subclass.\n",
      "\n",
      "        Args:\n",
      "            token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      "            token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[int]`: The model input with special tokens.\n",
      "        \"\"\"\n",
      "        if token_ids_1 is None:\n",
      "            return token_ids_0\n",
      "        return token_ids_0 + token_ids_1\n",
      "\n",
      "    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n",
      "    def prepare_for_model(\n",
      "        self,\n",
      "        ids: List[int],\n",
      "        pair_ids: Optional[List[int]] = None,\n",
      "        add_special_tokens: bool = True,\n",
      "        padding: Union[bool, str, PaddingStrategy] = False,\n",
      "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
      "        max_length: Optional[int] = None,\n",
      "        stride: int = 0,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        return_token_type_ids: Optional[bool] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "        return_overflowing_tokens: bool = False,\n",
      "        return_special_tokens_mask: bool = False,\n",
      "        return_offsets_mapping: bool = False,\n",
      "        return_length: bool = False,\n",
      "        verbose: bool = True,\n",
      "        prepend_batch_axis: bool = False,\n",
      "        **kwargs\n",
      "    ) -> BatchEncoding:\n",
      "        \"\"\"\n",
      "        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      "        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      "        manages a moving window (with user defined stride) for overflowing tokens\n",
      "\n",
      "        Args:\n",
      "            ids (:obj:`List[int]`):\n",
      "                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      "                and ``convert_tokens_to_ids`` methods.\n",
      "            pair_ids (:obj:`List[int]`, `optional`):\n",
      "                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      "                and ``convert_tokens_to_ids`` methods.\n",
      "        \"\"\"\n",
      "\n",
      "        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\n",
      "        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
      "            padding=padding,\n",
      "            truncation=truncation,\n",
      "            max_length=max_length,\n",
      "            pad_to_multiple_of=pad_to_multiple_of,\n",
      "            verbose=verbose,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        pair = bool(pair_ids is not None)\n",
      "        len_ids = len(ids)\n",
      "        len_pair_ids = len(pair_ids) if pair else 0\n",
      "\n",
      "        if return_token_type_ids and not add_special_tokens:\n",
      "            raise ValueError(\n",
      "                \"Asking to return token_type_ids while setting add_special_tokens to False \"\n",
      "                \"results in an undefined behavior. Please set add_special_tokens to True or \"\n",
      "                \"set return_token_type_ids to None.\"\n",
      "            )\n",
      "\n",
      "        # Load from model defaults\n",
      "        if return_token_type_ids is None:\n",
      "            return_token_type_ids = \"token_type_ids\" in self.model_input_names\n",
      "        if return_attention_mask is None:\n",
      "            return_attention_mask = \"attention_mask\" in self.model_input_names\n",
      "\n",
      "        encoded_inputs = {}\n",
      "\n",
      "        # Compute the total size of the returned encodings\n",
      "        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)\n",
      "\n",
      "        # Truncation: Handle max sequence length\n",
      "        overflowing_tokens = []\n",
      "        if truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len > max_length:\n",
      "            ids, pair_ids, overflowing_tokens = self.truncate_sequences(\n",
      "                ids,\n",
      "                pair_ids=pair_ids,\n",
      "                num_tokens_to_remove=total_len - max_length,\n",
      "                truncation_strategy=truncation_strategy,\n",
      "                stride=stride,\n",
      "            )\n",
      "\n",
      "        if return_overflowing_tokens:\n",
      "            encoded_inputs[\"overflowing_tokens\"] = overflowing_tokens\n",
      "            encoded_inputs[\"num_truncated_tokens\"] = total_len - max_length\n",
      "\n",
      "        # Add special tokens\n",
      "        if add_special_tokens:\n",
      "            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)\n",
      "            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)\n",
      "        else:\n",
      "            sequence = ids + pair_ids if pair else ids\n",
      "            token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])\n",
      "\n",
      "        # Build output dictionary\n",
      "        encoded_inputs[\"input_ids\"] = sequence\n",
      "        if return_token_type_ids:\n",
      "            encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
      "        if return_special_tokens_mask:\n",
      "            if add_special_tokens:\n",
      "                encoded_inputs[\"special_tokens_mask\"] = self.get_special_tokens_mask(ids, pair_ids)\n",
      "            else:\n",
      "                encoded_inputs[\"special_tokens_mask\"] = [0] * len(sequence)\n",
      "\n",
      "        # Check lengths\n",
      "        self._eventual_warn_about_too_long_sequence(encoded_inputs[\"input_ids\"], max_length, verbose)\n",
      "\n",
      "        # Padding\n",
      "        if padding_strategy != PaddingStrategy.DO_NOT_PAD or return_attention_mask:\n",
      "            encoded_inputs = self.pad(\n",
      "                encoded_inputs,\n",
      "                max_length=max_length,\n",
      "                padding=padding_strategy.value,\n",
      "                pad_to_multiple_of=pad_to_multiple_of,\n",
      "                return_attention_mask=return_attention_mask,\n",
      "            )\n",
      "\n",
      "        if return_length:\n",
      "            encoded_inputs[\"length\"] = len(encoded_inputs[\"input_ids\"])\n",
      "\n",
      "        batch_outputs = BatchEncoding(\n",
      "            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis\n",
      "        )\n",
      "\n",
      "        return batch_outputs\n",
      "\n",
      "    def truncate_sequences(\n",
      "        self,\n",
      "        ids: List[int],\n",
      "        pair_ids: Optional[List[int]] = None,\n",
      "        num_tokens_to_remove: int = 0,\n",
      "        truncation_strategy: Union[str, TruncationStrategy] = \"longest_first\",\n",
      "        stride: int = 0,\n",
      "    ) -> Tuple[List[int], List[int], List[int]]:\n",
      "        \"\"\"\n",
      "        Truncates a sequence pair in-place following the strategy.\n",
      "\n",
      "        Args:\n",
      "            ids (:obj:`List[int]`):\n",
      "                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      "                and ``convert_tokens_to_ids`` methods.\n",
      "            pair_ids (:obj:`List[int]`, `optional`):\n",
      "                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      "                and ``convert_tokens_to_ids`` methods.\n",
      "            num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      "                Number of tokens to remove using the truncation strategy.\n",
      "            truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      "                The strategy to follow for truncation. Can be:\n",
      "\n",
      "                * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      "                  to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "                  truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "                  sequences (or a batch of pairs) is provided.\n",
      "                * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      "                  the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "                * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      "                  to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "                * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "                  greater than the model maximum admissible input size).\n",
      "            stride (:obj:`int`, `optional`, defaults to 0):\n",
      "                If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      "                sequence returned. The value of this argument defines the number of additional tokens.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the\n",
      "            list of overflowing tokens.\n",
      "        \"\"\"\n",
      "        if num_tokens_to_remove <= 0:\n",
      "            return ids, pair_ids, []\n",
      "\n",
      "        if not isinstance(truncation_strategy, TruncationStrategy):\n",
      "            truncation_strategy = TruncationStrategy(truncation_strategy)\n",
      "\n",
      "        overflowing_tokens = []\n",
      "        if truncation_strategy == TruncationStrategy.LONGEST_FIRST:\n",
      "            for _ in range(num_tokens_to_remove):\n",
      "                if pair_ids is None or len(ids) > len(pair_ids):\n",
      "                    if not overflowing_tokens:\n",
      "                        window_len = min(len(ids), stride + 1)\n",
      "                    else:\n",
      "                        window_len = 1\n",
      "                    overflowing_tokens.extend(ids[-window_len:])\n",
      "                    ids = ids[:-1]\n",
      "                else:\n",
      "                    if not overflowing_tokens:\n",
      "                        window_len = min(len(pair_ids), stride + 1)\n",
      "                    else:\n",
      "                        window_len = 1\n",
      "                    overflowing_tokens.extend(pair_ids[-window_len:])\n",
      "                    pair_ids = pair_ids[:-1]\n",
      "        elif truncation_strategy == TruncationStrategy.ONLY_FIRST:\n",
      "            if len(ids) > num_tokens_to_remove:\n",
      "                window_len = min(len(ids), stride + num_tokens_to_remove)\n",
      "                overflowing_tokens = ids[-window_len:]\n",
      "                ids = ids[:-num_tokens_to_remove]\n",
      "            else:\n",
      "                logger.error(\n",
      "                    f\"We need to remove {num_tokens_to_remove} to truncate the input\"\n",
      "                    f\"but the first sequence has a length {len(ids)}. \"\n",
      "                    f\"Please select another truncation strategy than {truncation_strategy}, \"\n",
      "                    f\"for instance 'longest_first' or 'only_second'.\"\n",
      "                )\n",
      "        elif truncation_strategy == TruncationStrategy.ONLY_SECOND and pair_ids is not None:\n",
      "            if len(pair_ids) > num_tokens_to_remove:\n",
      "                window_len = min(len(pair_ids), stride + num_tokens_to_remove)\n",
      "                overflowing_tokens = pair_ids[-window_len:]\n",
      "                pair_ids = pair_ids[:-num_tokens_to_remove]\n",
      "            else:\n",
      "                logger.error(\n",
      "                    f\"We need to remove {num_tokens_to_remove} to truncate the input\"\n",
      "                    f\"but the second sequence has a length {len(pair_ids)}. \"\n",
      "                    f\"Please select another truncation strategy than {truncation_strategy}, \"\n",
      "                    f\"for instance 'longest_first' or 'only_first'.\"\n",
      "                )\n",
      "\n",
      "        return (ids, pair_ids, overflowing_tokens)\n",
      "\n",
      "    def _pad(\n",
      "        self,\n",
      "        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],\n",
      "        max_length: Optional[int] = None,\n",
      "        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
      "        pad_to_multiple_of: Optional[int] = None,\n",
      "        return_attention_mask: Optional[bool] = None,\n",
      "    ) -> dict:\n",
      "        \"\"\"\n",
      "        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)\n",
      "\n",
      "        Args:\n",
      "            encoded_inputs: Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).\n",
      "            max_length: maximum length of the returned list and optionally padding length (see below).\n",
      "                Will truncate by taking into account the special tokens.\n",
      "            padding_strategy: PaddingStrategy to use for padding.\n",
      "\n",
      "                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch\n",
      "                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)\n",
      "                - PaddingStrategy.DO_NOT_PAD: Do not pad\n",
      "                The tokenizer padding sides are defined in self.padding_side:\n",
      "\n",
      "                    - 'left': pads on the left of the sequences\n",
      "                    - 'right': pads on the right of the sequences\n",
      "            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n",
      "                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n",
      "                >= 7.5 (Volta).\n",
      "            return_attention_mask: (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n",
      "        \"\"\"\n",
      "        # Load from model defaults\n",
      "        if return_attention_mask is None:\n",
      "            return_attention_mask = \"attention_mask\" in self.model_input_names\n",
      "\n",
      "        required_input = encoded_inputs[self.model_input_names[0]]\n",
      "\n",
      "        if padding_strategy == PaddingStrategy.LONGEST:\n",
      "            max_length = len(required_input)\n",
      "\n",
      "        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
      "            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
      "\n",
      "        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length\n",
      "\n",
      "        if needs_to_be_padded:\n",
      "            difference = max_length - len(required_input)\n",
      "            if self.padding_side == \"right\":\n",
      "                if return_attention_mask:\n",
      "                    encoded_inputs[\"attention_mask\"] = [1] * len(required_input) + [0] * difference\n",
      "                if \"token_type_ids\" in encoded_inputs:\n",
      "                    encoded_inputs[\"token_type_ids\"] = (\n",
      "                        encoded_inputs[\"token_type_ids\"] + [self.pad_token_type_id] * difference\n",
      "                    )\n",
      "                if \"special_tokens_mask\" in encoded_inputs:\n",
      "                    encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n",
      "                encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n",
      "            elif self.padding_side == \"left\":\n",
      "                if return_attention_mask:\n",
      "                    encoded_inputs[\"attention_mask\"] = [0] * difference + [1] * len(required_input)\n",
      "                if \"token_type_ids\" in encoded_inputs:\n",
      "                    encoded_inputs[\"token_type_ids\"] = [self.pad_token_type_id] * difference + encoded_inputs[\n",
      "                        \"token_type_ids\"\n",
      "                    ]\n",
      "                if \"special_tokens_mask\" in encoded_inputs:\n",
      "                    encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n",
      "                encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n",
      "            else:\n",
      "                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n",
      "        elif return_attention_mask and \"attention_mask\" not in encoded_inputs:\n",
      "            encoded_inputs[\"attention_mask\"] = [1] * len(required_input)\n",
      "\n",
      "        return encoded_inputs\n",
      "\n",
      "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
      "        \"\"\"\n",
      "        Converts a sequence of tokens in a single string. The most simple way to do it is ``\" \".join(tokens)`` but we\n",
      "        often want to remove sub-word tokenization artifacts at the same time.\n",
      "\n",
      "        Args:\n",
      "            tokens (:obj:`List[str]`): The token to join in a string.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`str`: The joined tokens.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def batch_decode(\n",
      "        self,\n",
      "        sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n",
      "        skip_special_tokens: bool = False,\n",
      "        clean_up_tokenization_spaces: bool = True,\n",
      "        **kwargs\n",
      "    ) -> List[str]:\n",
      "        \"\"\"\n",
      "        Convert a list of lists of token ids into a list of strings by calling decode.\n",
      "\n",
      "        Args:\n",
      "            sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      "                List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      "            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to remove special tokens in the decoding.\n",
      "            clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "                Whether or not to clean up the tokenization spaces.\n",
      "            kwargs (additional keyword arguments, `optional`):\n",
      "                Will be passed to the underlying model specific decode method.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`List[str]`: The list of decoded sentences.\n",
      "        \"\"\"\n",
      "        return [\n",
      "            self.decode(\n",
      "                seq,\n",
      "                skip_special_tokens=skip_special_tokens,\n",
      "                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
      "                **kwargs,\n",
      "            )\n",
      "            for seq in sequences\n",
      "        ]\n",
      "\n",
      "    def decode(\n",
      "        self,\n",
      "        token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n",
      "        skip_special_tokens: bool = False,\n",
      "        clean_up_tokenization_spaces: bool = True,\n",
      "        **kwargs\n",
      "    ) -> str:\n",
      "        \"\"\"\n",
      "        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      "        tokens and clean up tokenization spaces.\n",
      "\n",
      "        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      "\n",
      "        Args:\n",
      "            token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      "                List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      "            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to remove special tokens in the decoding.\n",
      "            clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "                Whether or not to clean up the tokenization spaces.\n",
      "            kwargs (additional keyword arguments, `optional`):\n",
      "                Will be passed to the underlying model specific decode method.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`str`: The decoded sentence.\n",
      "        \"\"\"\n",
      "        # Convert inputs to python lists\n",
      "        token_ids = to_py_obj(token_ids)\n",
      "\n",
      "        return self._decode(\n",
      "            token_ids=token_ids,\n",
      "            skip_special_tokens=skip_special_tokens,\n",
      "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "    def _decode(\n",
      "        self,\n",
      "        token_ids: Union[int, List[int]],\n",
      "        skip_special_tokens: bool = False,\n",
      "        clean_up_tokenization_spaces: bool = True,\n",
      "        **kwargs\n",
      "    ) -> str:\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def get_special_tokens_mask(\n",
      "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
      "    ) -> List[int]:\n",
      "        \"\"\"\n",
      "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
      "\n",
      "        Args:\n",
      "            token_ids_0 (:obj:`List[int]`):\n",
      "                List of ids of the first sequence.\n",
      "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
      "                List of ids of the second sequence.\n",
      "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not the token list is already formatted with special tokens for the model.\n",
      "\n",
      "        Returns:\n",
      "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      "        \"\"\"\n",
      "        assert already_has_special_tokens and token_ids_1 is None, (\n",
      "            \"You cannot use ``already_has_special_tokens=False`` with this tokenizer. \"\n",
      "            \"Please use a slow (full python) tokenizer to activate this argument.\"\n",
      "            \"Or set `return_special_tokens_mask=True` when calling the encoding method \"\n",
      "            \"to get the special tokens mask in any tokenizer. \"\n",
      "        )\n",
      "\n",
      "        all_special_ids = self.all_special_ids  # cache the property\n",
      "\n",
      "        special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]\n",
      "\n",
      "        return special_tokens_mask\n",
      "\n",
      "    @staticmethod\n",
      "    def clean_up_tokenization(out_string: str) -> str:\n",
      "        \"\"\"\n",
      "        Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      "\n",
      "        Args:\n",
      "            out_string (:obj:`str`): The text to clean up.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`str`: The cleaned-up string.\n",
      "        \"\"\"\n",
      "        out_string = (\n",
      "            out_string.replace(\" .\", \".\")\n",
      "            .replace(\" ?\", \"?\")\n",
      "            .replace(\" !\", \"!\")\n",
      "            .replace(\" ,\", \",\")\n",
      "            .replace(\" ' \", \"'\")\n",
      "            .replace(\" n't\", \"n't\")\n",
      "            .replace(\" 'm\", \"'m\")\n",
      "            .replace(\" 's\", \"'s\")\n",
      "            .replace(\" 've\", \"'ve\")\n",
      "            .replace(\" 're\", \"'re\")\n",
      "        )\n",
      "        return out_string\n",
      "\n",
      "    def _eventual_warn_about_too_long_sequence(self, ids: List[int], max_length: Optional[int], verbose: bool):\n",
      "        \"\"\"\n",
      "        Depending on the input and internal state we might trigger a warning about a sequence that is too long for it's\n",
      "        corresponding model\n",
      "\n",
      "        Args:\n",
      "            ids (:obj:`List[str]`): The ids produced by the tokenization\n",
      "            max_length (:obj:`int`, `optional`): The max_length desired (does not trigger a warning if it is set)\n",
      "            verbose (:obj:`bool`): Whether or not to print more information and warnings.\n",
      "\n",
      "        \"\"\"\n",
      "        if max_length is None and len(ids) > self.model_max_length and verbose:\n",
      "            if not self.deprecation_warnings.get(\"sequence-length-is-longer-than-the-specified-maximum\", False):\n",
      "                logger.warning(\n",
      "                    \"Token indices sequence length is longer than the specified maximum sequence length \"\n",
      "                    \"for this model ({} > {}). Running this sequence through the model will result in \"\n",
      "                    \"indexing errors\".format(len(ids), self.model_max_length)\n",
      "                )\n",
      "            self.deprecation_warnings[\"sequence-length-is-longer-than-the-specified-maximum\"] = True\n",
      "\n",
      "    @contextmanager\n",
      "    def as_target_tokenizer(self):\n",
      "        \"\"\"\n",
      "        Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      "        sequence-to-sequence models that need a slightly different processing for the labels.\n",
      "        \"\"\"\n",
      "        yield\n",
      "\n",
      "    def prepare_seq2seq_batch(\n",
      "        self,\n",
      "        src_texts: List[str],\n",
      "        tgt_texts: Optional[List[str]] = None,\n",
      "        max_length: Optional[int] = None,\n",
      "        max_target_length: Optional[int] = None,\n",
      "        padding: str = \"longest\",\n",
      "        return_tensors: str = None,\n",
      "        truncation: bool = True,\n",
      "        **kwargs,\n",
      "    ) -> BatchEncoding:\n",
      "        \"\"\"\n",
      "        Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      "\n",
      "        Arguments:\n",
      "            src_texts (:obj:`List[str]`):\n",
      "                List of documents to summarize or source language texts.\n",
      "            tgt_texts (:obj:`list`, `optional`):\n",
      "                List of summaries or target language texts.\n",
      "            max_length (:obj:`int`, `optional`):\n",
      "                Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      "                left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length\n",
      "                is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "                length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "            max_target_length (:obj:`int`, `optional`):\n",
      "                Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      "                to :obj:`None`, this will use the max_length value.\n",
      "            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      "                Activates and controls padding. Accepts the following values:\n",
      "\n",
      "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      "                  single sequence if provided).\n",
      "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      "                  maximum acceptable input length for the model if that argument is not provided.\n",
      "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      "                  different lengths).\n",
      "            return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      "                If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "                * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      "                * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      "                * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      "            truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      "                Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "                * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      "                  :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      "                  provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      "                  if a pair of sequences (or a batch of pairs) is provided.\n",
      "                * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      "                  the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "                * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      "                  to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "                * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      "                  sequence lengths greater than the model maximum admissible input size).\n",
      "            **kwargs:\n",
      "                Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      "\n",
      "        Return:\n",
      "            :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      "\n",
      "            - **input_ids** -- List of token ids to be fed to the encoder.\n",
      "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      "            - **labels** -- List of token ids for tgt_texts.\n",
      "\n",
      "            The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.\n",
      "            Otherwise, input_ids, attention_mask will be the only keys.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the \"\n",
      "            \"regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` \"\n",
      "            \"context manager to prepare your targets. See the documentation of your specific tokenizer for more \"\n",
      "            \"details\",\n",
      "            FutureWarning,\n",
      "        )\n",
      "        # mBART-specific kwargs that should be ignored by other models.\n",
      "        kwargs.pop(\"src_lang\", None)\n",
      "        kwargs.pop(\"tgt_lang\", None)\n",
      "        if max_length is None:\n",
      "            max_length = self.model_max_length\n",
      "        model_inputs = self(\n",
      "            src_texts,\n",
      "            add_special_tokens=True,\n",
      "            return_tensors=return_tensors,\n",
      "            max_length=max_length,\n",
      "            padding=padding,\n",
      "            truncation=truncation,\n",
      "            **kwargs,\n",
      "        )\n",
      "        if tgt_texts is None:\n",
      "            return model_inputs\n",
      "        # Process tgt_texts\n",
      "        if max_target_length is None:\n",
      "            max_target_length = max_length\n",
      "        with self.as_target_tokenizer():\n",
      "            labels = self(\n",
      "                tgt_texts,\n",
      "                add_special_tokens=True,\n",
      "                return_tensors=return_tensors,\n",
      "                padding=padding,\n",
      "                max_length=max_target_length,\n",
      "                truncation=truncation,\n",
      "                **kwargs,\n",
      "            )\n",
      "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "        return model_inputs\n",
      "**************************************************\n",
      "A strategy that truncates a sequence of tokens.\n",
      "class TruncationStrategy(ExplicitEnum):\n",
      "    \"\"\"\n",
      "    Possible values for the ``truncation`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for\n",
      "    tab-completion in an IDE.\n",
      "    \"\"\"\n",
      "\n",
      "    ONLY_FIRST = \"only_first\"\n",
      "    ONLY_SECOND = \"only_second\"\n",
      "    LONGEST_FIRST = \"longest_first\"\n",
      "    DO_NOT_TRUNCATE = \"do_not_truncate\"\n",
      "**************************************************\n",
      "Return the default logging level.\n",
      "def _get_default_logging_level():\n",
      "def _get_library_name() -> str:\n",
      "def _get_library_root_logger() -> logging.Logger:\n",
      "def _configure_library_root_logger() -> None:\n",
      "def _reset_library_root_logger() -> None:\n",
      "def get_logger(name: Optional[str] = None) -> logging.Logger:\n",
      "def get_verbosity() -> int:\n",
      "def set_verbosity(verbosity: int) -> None:\n",
      "def set_verbosity_info():\n",
      "def set_verbosity_warning():\n",
      "def set_verbosity_debug():\n",
      "def set_verbosity_error():\n",
      "def disable_default_handler() -> None:\n",
      "def enable_default_handler() -> None:\n",
      "def add_handler(handler: logging.Handler) -> None:\n",
      "def remove_handler(handler: logging.Handler) -> None:\n",
      "def disable_propagation() -> None:\n",
      "def enable_propagation() -> None:\n",
      "def enable_explicit_format() -> None:\n",
      "def reset_format() -> None:\n",
      "**************************************************\n",
      "Base class for data processors for sequence classification data sets.\n",
      "class DataProcessor:\n",
      "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
      "\n",
      "    def get_example_from_tensor_dict(self, tensor_dict):\n",
      "        \"\"\"\n",
      "        Gets an example from a dict with tensorflow tensors.\n",
      "\n",
      "        Args:\n",
      "            tensor_dict: Keys and values should match the corresponding Glue\n",
      "                tensorflow_dataset examples.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def get_train_examples(self, data_dir):\n",
      "        \"\"\"Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def get_dev_examples(self, data_dir):\n",
      "        \"\"\"Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def get_test_examples(self, data_dir):\n",
      "        \"\"\"Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def get_labels(self):\n",
      "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def tfds_map(self, example):\n",
      "        \"\"\"\n",
      "        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts\n",
      "        examples to the correct format.\n",
      "        \"\"\"\n",
      "        if len(self.get_labels()) > 1:\n",
      "            example.label = self.get_labels()[int(example.label)]\n",
      "        return example\n",
      "\n",
      "    @classmethod\n",
      "    def _read_tsv(cls, input_file, quotechar=None):\n",
      "        \"\"\"Reads a tab separated value file.\"\"\"\n",
      "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
      "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for nl, code in zip(sample['nl_context'], sample['context']):\n",
    "    print(nl)\n",
    "    print(code)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
