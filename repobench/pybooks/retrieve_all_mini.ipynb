{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sir_timio/.local/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import load_data, replace_class_and_function_names, remove_docstrings\n",
    "from metrics import accuracy_at_k\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(preds, gts, ks=[1, 3, 5]):\n",
    "    for k in ks:\n",
    "        print(f'accuracy@{k}: {accuracy_at_k(preds, gts, k=k)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# settings :`cross_file_first`, `cross_file_random`, or `in_file`\n",
    "settings = 'cross_file_first'\n",
    "data = load_data('train', 'r', 'python', settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_samples = np.random.choice(data['easy'], n_samples)\n",
    "# raw_samples = data['easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:51<00:00, 19.35it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(sample['code'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode(sample['context'], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symm\n",
      "accuracy@1: 0.78\n",
      "accuracy@3: 0.922\n",
      "accuracy@5: 0.976\n",
      "assym\n",
      "accuracy@1: 0.188\n",
      "accuracy@3: 0.511\n",
      "accuracy@5: 0.828\n"
     ]
    }
   ],
   "source": [
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wihout docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:50<00:00, 19.97it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(remove_docstrings(sample['code']), convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode([remove_docstrings(c) for c in  sample['context']], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without docstrings\n",
      "symm\n",
      "accuracy@1: 0.773\n",
      "accuracy@3: 0.922\n",
      "accuracy@5: 0.98\n",
      "assym\n",
      "accuracy@1: 0.17\n",
      "accuracy@3: 0.489\n",
      "accuracy@5: 0.83\n"
     ]
    }
   ],
   "source": [
    "print('without docstrings')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def func():\n",
      "def func(x, partitioning_dims: int):\n",
      "def func(\n",
      "    x: T,\n",
      "    activation_partitioning_dims: Optional[int],\n",
      "    logical_axis_names: Tuple[str, ...],\n",
      ") -> T:\n",
      "T = TypeVar('T')\n",
      "--------------------------------------------------\n",
      "class cls(nn.Module, param_remapping.ParameterRemappable):\n",
      "class cls:\n",
      "  def func(self):\n",
      "  def __call__(self, inputs: Array, *args, **kwargs) -> Array:\n",
      "  def func(self, start_idx: int, end_idx: Optional[int],\n",
      "                            inputs: Array, *args, **kwargs) -> Array:\n",
      "  def __call__(self, inputs: Array, *args, **kwargs) -> Array:\n",
      "  def func(self, start_idx: int, end_idx: Optional[int],\n",
      "                            inputs: Array, *args, **kwargs) -> Array:\n",
      "--------------------------------------------------\n",
      "class cls(ClientMixin):\n",
      "    \"\"\"\n",
      "    The representation of a job on a remote hoplite server\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, address, port=5000, name=\"\", uuid=\"\", api_key=\"\", config={}):\n",
      "        \"\"\"\n",
      "        :param address: IP address or hostname of the computer running the job.\n",
      "            If desired, the address may be in the form \"address:port\", rather\n",
      "            than specifying the port in the second parameter\n",
      "        :param port: Port of the remote computer hoplite is listening on. This\n",
      "            is ignored if the address includes the port\n",
      "        :param name: qualified name of the job to run\n",
      "        :param uuid: the uuid of the job. If left blank then job will be\n",
      "            created by the daemon running on addr\n",
      "        :type uuid: string or None\n",
      "        :param api_key: this is used to only allow the running job to update\n",
      "            its own status\n",
      "        :raises: InvalidAddressError\n",
      "        :raises: JobDoesNotExistError\n",
      "        :raises: ConnectionError\n",
      "        \"\"\"\n",
      "        if ':' in address:\n",
      "            self.address = address.split(':')[0]\n",
      "            self.port = address.split(':')[1]\n",
      "        else:\n",
      "            self.address = address\n",
      "            self.port = port\n",
      "        self._daemon_addr = 'http://{0}:{1}'.format(self.address, self.port)\n",
      "        self._config = config\n",
      "        self.name = name\n",
      "        self.uuid = uuid\n",
      "        self._api_key = api_key\n",
      "        self._last_poll = 0\n",
      "\n",
      "        try:\n",
      "            if not self.uuid:\n",
      "                self._create_job()\n",
      "            self._get_job()\n",
      "        except requests.exceptions.ConnectionError:\n",
      "            raise ConnectionError(self.address)\n",
      "\n",
      "    def func(self, force=False):\n",
      "        \"\"\"\n",
      "        Get the config dictionary for this job\n",
      "\n",
      "        :return: the configuration dictionary the job was created with\n",
      "        :rtype: dict\n",
      "        \"\"\"\n",
      "        self._get_job(force)\n",
      "        return self._config\n",
      "\n",
      "    def func(self, force=False):\n",
      "        \"\"\"\n",
      "        Get the status dictionary of the job\n",
      "\n",
      "        :return: the status dictionary of the job\n",
      "        :rtype: dict\n",
      "        :raises JobFailedError: if the job raised an exception\n",
      "        \"\"\"\n",
      "        self._get_job(force)\n",
      "        exception_dict = self._status.get(\"exception\", None)\n",
      "        if exception_dict:\n",
      "            # Raise an exception, being sure to store the information from the\n",
      "            # previous exception. This is so that stack traces for every job\n",
      "            # are preserved, in the event that jobs call other jobs which then\n",
      "            # throw exceptions.\n",
      "            raise JobFailedError(\n",
      "                self.address,\n",
      "                self.uuid,\n",
      "                pickle.loads(exception_dict['traceback']),\n",
      "                exception_dict['previous_exception'])\n",
      "        return self._status\n",
      "\n",
      "    def func(self):\n",
      "        \"\"\"\n",
      "        Start the job\n",
      "\n",
      "        :return: true if the job was started successfully\n",
      "        :raises JobDoesNotExistError: Job does not exist on the server\n",
      "        \"\"\"\n",
      "        self._get_job()\n",
      "        resp = self.jput(\n",
      "            self._daemon_addr + '/jobs/{0}/start'.format(self.uuid))\n",
      "        return resp.json()[\"started\"]\n",
      "\n",
      "    def func(self, timeout=-1):\n",
      "        \"\"\"\n",
      "        This will block until the job is finished and then return True.\n",
      "        Timeout is in seconds.\n",
      "\n",
      "        Blocks infinitely by default (timeout=-1).\n",
      "\n",
      "        :raises TimeoutError: The specified timeout was reached\n",
      "        :raises JobFailedError: The job threw an exception\n",
      "        \"\"\"\n",
      "        num_seconds = 0\n",
      "        poll_interval = .05\n",
      "        while num_seconds < timeout or timeout == -1:\n",
      "            if self.finished():\n",
      "                return True\n",
      "            time.sleep(poll_interval)\n",
      "            num_seconds += poll_interval\n",
      "        raise TimeoutError(self.uuid)\n",
      "\n",
      "    def func(self, force=False):\n",
      "        \"\"\"\n",
      "        Kill the job\n",
      "\n",
      "        :return: true if the kill command was sent successfully. Success here\n",
      "            does not mean the job is stopped, it only means that a kill signal\n",
      "            was successfully sent to the job\n",
      "        :rtype: bool\n",
      "        :raises JobDoesNotExistError: if the job does not exist on the targeted\n",
      "            server\n",
      "        \"\"\"\n",
      "        self._get_job(force)\n",
      "        resp = self.jput(\n",
      "            self._daemon_addr + '/jobs/{0}/kill'.format(self.uuid))\n",
      "        return hoplite_loads(str(resp.text))[\"killed\"]\n",
      "\n",
      "    def func(self, force=False):\n",
      "        \"\"\"\n",
      "        :return: true if the job is currently executing on the target machine\n",
      "        :rtype: bool\n",
      "        :raises JobDoesNotExistError: Job not found on the server\n",
      "        \"\"\"\n",
      "        self._get_job(force)\n",
      "        return self._running\n",
      "\n",
      "    def func(self, force=False):\n",
      "        \"\"\"\n",
      "        Returns true if the job on the target machine is no longer executing\n",
      "\n",
      "        :return: If the job has run and is currently not running\n",
      "        :rtype: bool\n",
      "        :raises JobDoesNotExistError: Job not found on the server\n",
      "        :raises JobFailedError: Job raised an exception\n",
      "        \"\"\"\n",
      "        self.status(force)\n",
      "        return self._finished\n",
      "\n",
      "    def func(self, force=False):\n",
      "        \"\"\"\n",
      "\n",
      "        I call this before most other requests to get the status code sanity\n",
      "        check.\n",
      "        This method is rate limited for sanity\n",
      "        \"\"\"\n",
      "        time_elapsed = time.time() - self._last_poll\n",
      "        if time_elapsed > .2 or force:\n",
      "            resp = self.jget(self._daemon_addr + '/jobs/{0}'.format(self.uuid))\n",
      "            if resp.status_code == 404:\n",
      "                raise JobDoesNotExistError\n",
      "            self._set_attributes_from_response_json(\n",
      "                hoplite_loads(str(resp.text)))\n",
      "            self._last_poll = time.time()\n",
      "\n",
      "    def func(self):\n",
      "        job_data = {\"name\": self.name, \"config\": self._config, \"port\": self.port}\n",
      "        resp = self.jpost(self._daemon_addr + '/jobs', data=job_data)\n",
      "        if resp.status_code == 400:\n",
      "            raise JobDoesNotExistError(hoplite_loads(str(resp.text))[\"error\"])\n",
      "        self._set_attributes_from_response_json(hoplite_loads(str(resp.text)))\n",
      "\n",
      "    def func(self, resp_dict):\n",
      "        job = resp_dict\n",
      "        self.uuid = job[\"uuid\"]\n",
      "        self.name = job[\"name\"]\n",
      "        self._status = job.get(\"status\", {})\n",
      "        self._config = job.get(\"config\", {})\n",
      "        self._running = job.get(\"running\", False)\n",
      "        self._finished = job.get(\"finished\", False)\n",
      "--------------------------------------------------\n",
      "class cls(HopliteError):\n",
      "    \"\"\"\n",
      "    Raised when there is an \"exception\" key in a job's status on the client.\n",
      "\n",
      "    This exception can contain nested information regarding other exceptions\n",
      "    which preceded it. Specifically, in cases where jobs call other jobs which\n",
      "    raise exceptions, stack trace and other important information for every\n",
      "    intermediate job can be preserved in a single exception.  This is useful\n",
      "    for debugging in complex situations which require chaining of multiple\n",
      "    jobs.\n",
      "    \"\"\"\n",
      "    def __init__(self, address, uuid, traceback_object, previous_exception):\n",
      "        \"\"\"\n",
      "        :param String address: Address of the host that was running the job\n",
      "        :param String uuid: uuid of the job that threw the exception\n",
      "        :param [(String, String, String, String)] traceback_object: Traceback\n",
      "            object for the exception which occurred\n",
      "        :param previous_exception: The information from the previously-raised\n",
      "            exceptions\n",
      "        \"\"\"\n",
      "        self.addr = address\n",
      "        self.uuid = uuid\n",
      "        self.traceback_object = traceback_object\n",
      "        self.previous_exception = previous_exception\n",
      "\n",
      "    @property\n",
      "    def func(self):\n",
      "        \"\"\"\n",
      "        Ensure that full info about the exception will always be retrieved when\n",
      "        viewing the message\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        return self.__str__()\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"\n",
      "        Provides useful information when printing information about this\n",
      "        exception. This will display the full traces for each job which was\n",
      "        called leading up to the root exception.\n",
      "\n",
      "        :return: A string containing a full representation of all information\n",
      "            contained in this exception. This includes stack traces for all\n",
      "            jobs called before the exception occurred.\n",
      "        \"\"\"\n",
      "        def func(traceback_object):\n",
      "            trace_str = ''\n",
      "            for line in traceback.extract_tb(traceback_object):\n",
      "                trace_str += '      File \"{0}\", line {1}, in {2}'.format(line[0], line[1], line[2]) + '\\n'\n",
      "                trace_str += '         {0}'.format(line[3]) + '\\n'\n",
      "            return trace_str\n",
      "\n",
      "        output = ''\n",
      "        output += 'Full traceback for all jobs descended from current job:\\n'\n",
      "        output += '   In job with UUID: {}'.format(self.uuid) + '\\n'\n",
      "        output += '   Running on machine: {}'.format(self.addr) + '\\n'\n",
      "        output += '      Traceback:\\n'\n",
      "        output += single_traceback(self.traceback_object)\n",
      "\n",
      "        previous_exception = self.previous_exception\n",
      "        while (previous_exception is not None) and (previous_exception.get('type', None) is None):\n",
      "            output += 'In job with UUID: {}'.format(previous_exception.get('uuid')) + '\\n'\n",
      "            output += 'Running on machine: {}'.format(previous_exception.get('address')) + '\\n'\n",
      "            output += '   Traceback:\\n'\n",
      "            output += single_traceback(pickle.loads(previous_exception.get('traceback')))\n",
      "            previous_exception = previous_exception.get('previous_exception', None)\n",
      "\n",
      "        output += '   Root Error Type: {}'.format(previous_exception.get('type')) + '\\n'\n",
      "        output += '   Root Error Message: {}'.format(previous_exception.get('message'))\n",
      "        return output\n",
      "\n",
      "    def func(self):\n",
      "        \"\"\"\n",
      "        Handles JobFailedErrors by trying to raise the same exception which was\n",
      "        raised on the remote machine. This is useful in allowing a client to\n",
      "        operate on a remote machine more transparently (i.e. it seems more like\n",
      "        the operations are occurring on the local machine, since the exceptions\n",
      "        raised on the remote machine are also raised on the local machine).\n",
      "        This functionality is probably vulnerable to many corner cases, and\n",
      "        might not be particularly reliable.\n",
      "\n",
      "        Examines the JobFailedError exception information to determine what\n",
      "        exception was originally raised on the remote machine. If that\n",
      "        exception exists in scope on this machine (i.e. a currently-loaded\n",
      "        module defines that exception), then that exception will be raised on\n",
      "        the local machine. Otherwise, a JobFailedError exception will be\n",
      "        raised.\n",
      "\n",
      "        This is called as part of the RemotableMetaClass and Remotify\n",
      "        decorator, and should not be manually called in most circumstances\n",
      "\n",
      "        :raises JobFailedError: When the original exception cannot be raised\n",
      "            for any reason, then the JobFailedError from which this was called\n",
      "            is reraised.\n",
      "        :raises Various Other Errors: When a known exception occurred in the\n",
      "            remote job, then that exception will be raised\n",
      "        \"\"\"\n",
      "\n",
      "        # Get to bottom-most exception (the original exception that was thrown)\n",
      "        previous_exception = self.previous_exception\n",
      "        while (previous_exception is not None) and ('type' not in previous_exception):\n",
      "            previous_exception = previous_exception.get('previous_exception', None)\n",
      "\n",
      "        exception_object_string = previous_exception.get('exception_object', None)\n",
      "\n",
      "        exception_to_raise = self\n",
      "\n",
      "        try:\n",
      "            if exception_object_string:\n",
      "                exception_object = pickle.loads(exception_object_string)\n",
      "                if exception_object is not None:\n",
      "                    exception_to_raise = exception_object\n",
      "        except Exception as e:  # If unable to raise original exception, then reraise self\n",
      "            pass\n",
      "        finally:\n",
      "            logger.info('Exception raised from remote machine \"{}\". Local traceback:\\n{}\\n'\n",
      "                        'Remote exception info:\\n{}'.format(self.addr,\n",
      "                                                            traceback.print_tb(sys.exc_info()[2]),\n",
      "                                                            traceback.format_exception(\n",
      "                                                                type(exception_to_raise),\n",
      "                                                                exception_to_raise,\n",
      "                                                                self.traceback_object\n",
      "                                                            )))\n",
      "            raise type(exception_to_raise), exception_to_raise, self.traceback_object.tb_next.tb_next\n",
      "--------------------------------------------------\n",
      "class cls(object):\n",
      "    \"\"\"\n",
      "    Setup remote store and save config\n",
      "    \"\"\"\n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Pass in the config containing remote_store/url so we can access the remote data.\n",
      "        :param config: Config global configuration for use with this command.\n",
      "        \"\"\"\n",
      "        self.remote_store = RemoteStore(config)\n",
      "        self.config = config\n",
      "\n",
      "    @staticmethod\n",
      "    def func(args):\n",
      "        if args.project_name:\n",
      "            return ProjectNameOrId.create_from_name(args.project_name)\n",
      "        else:\n",
      "            return ProjectNameOrId.create_from_project_id(args.project_id)\n",
      "\n",
      "    def func(self, args, must_exist=True, include_children=False):\n",
      "        project_name_or_id = self.create_project_name_or_id_from_args(args)\n",
      "        if include_children:\n",
      "            print(\"Fetching list of files for project {}.\".format(project_name_or_id.value))\n",
      "        project = self.remote_store.fetch_remote_project(project_name_or_id,\n",
      "                                                         must_exist=must_exist,\n",
      "                                                         include_children=include_children)\n",
      "        if include_children:\n",
      "            print(\"Done fetching list of files.\")\n",
      "        return project\n",
      "\n",
      "    def func(self, emails, usernames):\n",
      "        \"\"\"\n",
      "        Given a list of emails and usernames fetch DukeDS user info.\n",
      "        Parameters that are None will be skipped.\n",
      "        :param emails: [str]: list of emails (can be null)\n",
      "        :param usernames:  [str]: list of usernames(netid)\n",
      "        :return: [RemoteUser]: details about any users referenced the two parameters\n",
      "        \"\"\"\n",
      "        to_users = []\n",
      "        if emails:\n",
      "            for email in emails:\n",
      "                user = self.remote_store.get_or_register_user_by_email(email)\n",
      "                to_users.append(user)\n",
      "        if usernames:\n",
      "            for username in usernames:\n",
      "                user = self.remote_store.get_or_register_user_by_username(username)\n",
      "                to_users.append(user)\n",
      "        return to_users\n",
      "\n",
      "    def func(self):\n",
      "        self.remote_store.close()\n",
      "--------------------------------------------------\n",
      "class cls(BaseCommand):\n",
      "    \"\"\"\n",
      "    Uploads a folder to a remote project.\n",
      "    \"\"\"\n",
      "    def __init__(self, config):\n",
      "        \"\"\"\n",
      "        Pass in the config containing remote_store/url so we can access the remote data.\n",
      "        :param config: Config global configuration for use with this command.\n",
      "        \"\"\"\n",
      "        super(UploadCommand, self).__init__(config)\n",
      "\n",
      "    def func(self, args):\n",
      "        \"\"\"\n",
      "        Upload contents of folders to a project with project_name on remote store.\n",
      "        If follow_symlinks we will traverse symlinked directories.\n",
      "        If content is already on remote site it will not be sent.\n",
      "        :param args: Namespace arguments parsed from the command line.\n",
      "        \"\"\"\n",
      "        project_name_or_id = self.create_project_name_or_id_from_args(args)\n",
      "        folders = args.folders                  # list of local files/folders to upload into the project\n",
      "        follow_symlinks = args.follow_symlinks  # should we follow symlinks when traversing folders\n",
      "        dry_run = args.dry_run                  # do not upload anything, instead print out what you would upload\n",
      "        check_file_consistency = args.check     # should we check download URLs after uploading\n",
      "\n",
      "        # Find files and folders to upload\n",
      "        local_project = LocalProject(followsymlinks=follow_symlinks, file_exclude_regex=self.config.file_exclude_regex)\n",
      "        local_project.add_paths(folders)\n",
      "        local_items_count = local_project.count_local_items()\n",
      "        print(local_items_count.to_str(prefix=\"Checking\"))\n",
      "\n",
      "        # Fetch remote project (if there is one) and update local_project with details from remote project\n",
      "        remote_project = self.remote_store.fetch_remote_project(project_name_or_id)\n",
      "        local_project.update_remote_ids(remote_project)\n",
      "        items_to_send_count = local_project.count_items_to_send(self.config.upload_bytes_per_chunk)\n",
      "        print(items_to_send_count.to_str(local_items_count=local_items_count, prefix=\"Synchronizing\"))\n",
      "\n",
      "        if dry_run:\n",
      "            # Check hashes to see what needs to be uploaded\n",
      "            dry_run = ProjectUploadDryRun(local_project)\n",
      "            print(dry_run.get_report())\n",
      "        else:\n",
      "            # Upload files and folders\n",
      "            project_upload = ProjectUpload(self.config, project_name_or_id, local_project, items_to_send_count)\n",
      "            project_upload.run()\n",
      "\n",
      "            # Show user results of upload\n",
      "            upload_report = project_upload.get_upload_report()\n",
      "            print(upload_report.summary())\n",
      "            print()\n",
      "            if upload_report.sent_data:\n",
      "                print('\\n')\n",
      "                print(upload_report.get_content())\n",
      "                print('\\n')\n",
      "            print(project_upload.get_url_msg())\n",
      "            project_upload.cleanup()\n",
      "\n",
      "            # check for consistency unless user passes --no-check flag\n",
      "            if check_file_consistency:\n",
      "                self.wait_for_consistency(local_project.remote_id)\n",
      "\n",
      "    def func(self, project_id):\n",
      "        client = Client(self.config)\n",
      "        project = client.get_project_by_id(project_id)\n",
      "        checker = ProjectChecker(self.config, project)\n",
      "        try:\n",
      "            checker.wait_for_consistency()\n",
      "        except DSHashMismatchError:\n",
      "            checker.print_bad_uploads_table()\n",
      "            sys.exit(1)\n",
      "        finally:\n",
      "            client.close()\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sample in raw_samples[:3]:\n",
    "    for cntx in sample['context'][:2]:\n",
    "        print(replace_class_and_function_names(cntx))\n",
    "        print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:54<00:00, 18.45it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(replace_class_and_function_names(sample['code']), convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode([replace_class_and_function_names(c) for c in  sample['context']], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without names\n",
      "symm\n",
      "accuracy@1: 0.561\n",
      "accuracy@3: 0.842\n",
      "accuracy@5: 0.964\n",
      "assym\n",
      "accuracy@1: 0.167\n",
      "accuracy@3: 0.487\n",
      "accuracy@5: 0.818\n"
     ]
    }
   ],
   "source": [
    "print('without names')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without docstring, renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:50<00:00, 19.65it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(remove_docstrings(replace_class_and_function_names(sample['code'])), convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode([remove_docstrings(replace_class_and_function_names(c)) for c in  sample['context']], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without names and docstrings\n",
      "symm\n",
      "accuracy@1: 0.512\n",
      "accuracy@3: 0.824\n",
      "accuracy@5: 0.957\n",
      "assym\n",
      "accuracy@1: 0.157\n",
      "accuracy@3: 0.477\n",
      "accuracy@5: 0.82\n"
     ]
    }
   ],
   "source": [
    "print('without names and docstrings')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code_elements(code):\n",
    "    \"\"\"\n",
    "    Extract class names, function names, class fields, and function arguments from the given code snippet.\n",
    "    \"\"\"\n",
    "    # Regular expressions for class and function definitions, class fields, and function arguments\n",
    "    class_pattern = r\"\\bclass\\s+(\\w+)\"\n",
    "    function_pattern = r\"\\bdef\\s+(\\w+)\"\n",
    "    class_field_pattern = r\"\\bself\\.(\\w+)\"\n",
    "    function_arg_pattern = r\"\\bdef\\s+\\w+\\(([^)]*)\\)\"\n",
    "    docstring_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''\n",
    "\n",
    "    # Extract class and function names\n",
    "    class_names = re.findall(class_pattern, code)\n",
    "    function_names = re.findall(function_pattern, code)\n",
    "\n",
    "    # Extract class fields and function arguments\n",
    "    class_fields = re.findall(class_field_pattern, code)\n",
    "    function_args = re.findall(function_arg_pattern, code)\n",
    "    \n",
    "    # Extract docstrings \n",
    "    docstrings = re.findall(docstring_pattern, code, re.DOTALL)\n",
    "\n",
    "    # Process function arguments to split them into individual arguments\n",
    "    processed_function_args = []\n",
    "    for args in function_args:\n",
    "        args = args.replace(' ', '').split(',')\n",
    "        # Remove 'self' from arguments\n",
    "        args = [arg for arg in args if arg != 'self' and arg]\n",
    "        args = [arg.split('=')[0].strip() for arg in args]\n",
    "        processed_function_args.extend(args)\n",
    "\n",
    "    # Create a dictionary with unique names\n",
    "    unique_names = {\n",
    "        \"class_names\": list(set(class_names)),\n",
    "        \"function_names\": list(set(function_names) - set(['__init__', '__str__', '__len__'])),\n",
    "        \"class_fields\": list(set(class_fields)),\n",
    "        \"function_args\": list(set(processed_function_args)),\n",
    "        \"docstrings\": docstrings,\n",
    "    }\n",
    "\n",
    "    return unique_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_names': ['BufferedStream'],\n",
       " 'function_names': ['_bytes_remaining',\n",
       "  'stream_end_position',\n",
       "  'read',\n",
       "  'stream_exhausted'],\n",
       " 'class_fields': [],\n",
       " 'function_args': ['start', 'stream', 'size']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = extract_code_elements(sample['context'][0])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:48<00:00, 20.44it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "\n",
    "        # raw code\n",
    "        code_embedding = model.encode(sample['code'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "\n",
    "        # only keywords\n",
    "        new_context = []\n",
    "        for c in sample['context']:\n",
    "            code_meta = extract_code_elements(c)\n",
    "            s = \"\"\n",
    "            s += \" \".join(code_meta['class_names']) + \" \"\n",
    "            s += \" \".join(code_meta['function_names']) + \" \"\n",
    "            s += \" \".join(code_meta['docstrings']) + \" \"\n",
    "            \n",
    "            new_context.append(s)\n",
    "        context_embedding = model.encode(new_context, batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context - only keywords and dosctrings\n",
      "symm\n",
      "accuracy@1: 0.723\n",
      "accuracy@3: 0.901\n",
      "accuracy@5: 0.97\n",
      "assym\n",
      "accuracy@1: 0.169\n",
      "accuracy@3: 0.51\n",
      "accuracy@5: 0.835\n"
     ]
    }
   ],
   "source": [
    "print('context - only keywords and dosctrings')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
