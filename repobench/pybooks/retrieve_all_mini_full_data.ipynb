{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sir_timio/.local/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import load_data, replace_class_and_function_names, remove_docstrings\n",
    "from metrics import accuracy_at_k\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(preds, gts, ks=[1, 3, 5]):\n",
    "    for k in ks:\n",
    "        print(f'accuracy@{k}: {accuracy_at_k(preds, gts, k=k)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# settings :`cross_file_first`, `cross_file_random`, or `in_file`\n",
    "settings = 'cross_file_first'\n",
    "data = load_data('train', 'r', 'python', settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_samples = np.random.choice(data['easy'], n_samples)\n",
    "raw_samples = data['easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98038/98038 [1:26:46<00:00, 18.83it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(sample['code'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode(sample['context'], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symm\n",
      "accuracy@1: 0.77697\n",
      "accuracy@3: 0.92849\n",
      "accuracy@5: 0.97564\n",
      "assym\n",
      "accuracy@1: 0.13946\n",
      "accuracy@3: 0.45182\n",
      "accuracy@5: 0.78131\n"
     ]
    }
   ],
   "source": [
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wihout docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98038/98038 [1:24:02<00:00, 19.44it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(remove_docstrings(sample['code']), convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode([remove_docstrings(c) for c in  sample['context']], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without docstrings\n",
      "symm\n",
      "accuracy@1: 0.77258\n",
      "accuracy@3: 0.92677\n",
      "accuracy@5: 0.9756\n",
      "assym\n",
      "accuracy@1: 0.13559\n",
      "accuracy@3: 0.44889\n",
      "accuracy@5: 0.78066\n"
     ]
    }
   ],
   "source": [
    "print('without docstrings')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def func(config=ProdConfig):\n",
      "    app = Flask(__name__)\n",
      "    app.config.from_object(config)\n",
      "    app.config.from_envvar('DOORMAN_SETTINGS', silent=True)\n",
      "\n",
      "    register_blueprints(app)\n",
      "    register_errorhandlers(app)\n",
      "    register_loggers(app)\n",
      "    register_extensions(app)\n",
      "    register_auth_method(app)\n",
      "    register_filters(app)\n",
      "\n",
      "    return app\n",
      "--------------------------------------------------\n",
      "class cls(object):\n",
      "class cls(CRUDMixin, db.Model):\n",
      "class cls(object):\n",
      "    def func(cls, **kwargs):\n",
      "    def func(self, commit=True, **kwargs):\n",
      "    def func(self, commit=True):\n",
      "    def func(self, commit=True):\n",
      "    def func(cls, record_id):\n",
      "def func(tablename, nullable=False, pk_name='id', **kwargs):\n",
      "--------------------------------------------------\n",
      "def func(config=ProdConfig):\n",
      "    app = Flask(__name__)\n",
      "    app.config.from_object(config)\n",
      "    app.config.from_envvar('DOORMAN_SETTINGS', silent=True)\n",
      "\n",
      "    register_blueprints(app)\n",
      "    register_errorhandlers(app)\n",
      "    register_loggers(app)\n",
      "    register_extensions(app)\n",
      "    register_auth_method(app)\n",
      "    register_filters(app)\n",
      "\n",
      "    return app\n",
      "--------------------------------------------------\n",
      "class cls(object):\n",
      "class cls(CRUDMixin, db.Model):\n",
      "class cls(object):\n",
      "    def func(cls, **kwargs):\n",
      "    def func(self, commit=True, **kwargs):\n",
      "    def func(self, commit=True):\n",
      "    def func(self, commit=True):\n",
      "    def func(cls, record_id):\n",
      "def func(tablename, nullable=False, pk_name='id', **kwargs):\n",
      "--------------------------------------------------\n",
      "def func(config=ProdConfig):\n",
      "    app = Flask(__name__)\n",
      "    app.config.from_object(config)\n",
      "    app.config.from_envvar('DOORMAN_SETTINGS', silent=True)\n",
      "\n",
      "    register_blueprints(app)\n",
      "    register_errorhandlers(app)\n",
      "    register_loggers(app)\n",
      "    register_extensions(app)\n",
      "    register_auth_method(app)\n",
      "    register_filters(app)\n",
      "\n",
      "    return app\n",
      "--------------------------------------------------\n",
      "class cls(object):\n",
      "class cls(CRUDMixin, db.Model):\n",
      "class cls(object):\n",
      "    def func(cls, **kwargs):\n",
      "    def func(self, commit=True, **kwargs):\n",
      "    def func(self, commit=True):\n",
      "    def func(self, commit=True):\n",
      "    def func(cls, record_id):\n",
      "def func(tablename, nullable=False, pk_name='id', **kwargs):\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sample in raw_samples[:3]:\n",
    "    for cntx in sample['context'][:2]:\n",
    "        print(replace_class_and_function_names(cntx))\n",
    "        print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98038/98038 [1:29:30<00:00, 18.25it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(replace_class_and_function_names(sample['code']), convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode([replace_class_and_function_names(c) for c in  sample['context']], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without names\n",
      "symm\n",
      "accuracy@1: 0.5793\n",
      "accuracy@3: 0.84291\n",
      "accuracy@5: 0.94924\n",
      "assym\n",
      "accuracy@1: 0.14272\n",
      "accuracy@3: 0.45467\n",
      "accuracy@5: 0.78229\n"
     ]
    }
   ],
   "source": [
    "print('without names')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without docstring, renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98038/98038 [1:25:27<00:00, 19.12it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        code_embedding = model.encode(remove_docstrings(replace_class_and_function_names(sample['code'])), convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        context_embedding = model.encode([remove_docstrings(replace_class_and_function_names(c)) for c in  sample['context']], batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without names and docstrings\n",
      "symm\n",
      "accuracy@1: 0.5295\n",
      "accuracy@3: 0.81043\n",
      "accuracy@5: 0.93949\n",
      "assym\n",
      "accuracy@1: 0.14221\n",
      "accuracy@3: 0.45543\n",
      "accuracy@5: 0.78327\n"
     ]
    }
   ],
   "source": [
    "print('without names and docstrings')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code_elements(code):\n",
    "    \"\"\"\n",
    "    Extract class names, function names, class fields, and function arguments from the given code snippet.\n",
    "    \"\"\"\n",
    "    # Regular expressions for class and function definitions, class fields, and function arguments\n",
    "    class_pattern = r\"\\bclass\\s+(\\w+)\"\n",
    "    function_pattern = r\"\\bdef\\s+(\\w+)\"\n",
    "    class_field_pattern = r\"\\bself\\.(\\w+)\"\n",
    "    function_arg_pattern = r\"\\bdef\\s+\\w+\\(([^)]*)\\)\"\n",
    "    docstring_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''\n",
    "\n",
    "    # Extract class and function names\n",
    "    class_names = re.findall(class_pattern, code)\n",
    "    function_names = re.findall(function_pattern, code)\n",
    "\n",
    "    # Extract class fields and function arguments\n",
    "    class_fields = re.findall(class_field_pattern, code)\n",
    "    function_args = re.findall(function_arg_pattern, code)\n",
    "    \n",
    "    # Extract docstrings \n",
    "    docstrings = re.findall(docstring_pattern, code, re.DOTALL)\n",
    "\n",
    "    # Process function arguments to split them into individual arguments\n",
    "    processed_function_args = []\n",
    "    for args in function_args:\n",
    "        args = args.replace(' ', '').split(',')\n",
    "        # Remove 'self' from arguments\n",
    "        args = [arg for arg in args if arg != 'self' and arg]\n",
    "        args = [arg.split('=')[0].strip() for arg in args]\n",
    "        processed_function_args.extend(args)\n",
    "\n",
    "    # Create a dictionary with unique names\n",
    "    unique_names = {\n",
    "        \"class_names\": list(set(class_names)),\n",
    "        \"function_names\": list(set(function_names) - set(['__init__', '__str__', '__len__'])),\n",
    "        \"class_fields\": list(set(class_fields)),\n",
    "        \"function_args\": list(set(processed_function_args)),\n",
    "        \"docstrings\": docstrings,\n",
    "    }\n",
    "\n",
    "    return unique_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_names': ['JobSpec'],\n",
       " 'function_names': ['add_workspec_list',\n",
       "  'all_files_triggered_to_stage_out',\n",
       "  'is_final_status',\n",
       "  'set_all_input_ready',\n",
       "  'get_pilot_type',\n",
       "  'all_events_done',\n",
       "  'get_groups_of_output_files',\n",
       "  'get_files_to_delete',\n",
       "  'has_attribute',\n",
       "  'get_job_status_from_attributes',\n",
       "  'get_output_file_attributes',\n",
       "  'get_job_params',\n",
       "  'update_group_status_in_files',\n",
       "  'add_out_file',\n",
       "  'trigger_preparation',\n",
       "  'set_input_file_paths',\n",
       "  'set_attributes',\n",
       "  'get_status',\n",
       "  'set_start_time',\n",
       "  'get_input_file_attributes',\n",
       "  'is_pilot_closed',\n",
       "  'add_in_file',\n",
       "  'not_suppress_heartbeat',\n",
       "  'get_output_file_specs',\n",
       "  'set_end_time',\n",
       "  'add_file',\n",
       "  'set_groups_to_files',\n",
       "  'get_input_file_specs',\n",
       "  'get_workspec_list',\n",
       "  'convert_job_json',\n",
       "  'trigger_stage_out',\n",
       "  'set_pilot_closed',\n",
       "  'trigger_propagation',\n",
       "  'to_event_data',\n",
       "  'get_logfile_info',\n",
       "  'add_event',\n",
       "  'get_groups_of_input_files',\n",
       "  'set_pilot_error',\n",
       "  'set_one_attribute',\n",
       "  'get_job_attributes_for_panda',\n",
       "  'reset_out_file',\n",
       "  'all_files_zipped'],\n",
       " 'class_fields': ['zipEventMap',\n",
       "  'is_final_status',\n",
       "  'jobParams',\n",
       "  'workspec_list',\n",
       "  'all_events_done',\n",
       "  'zipPerMB',\n",
       "  'jobAttributes',\n",
       "  'has_attribute',\n",
       "  'outFiles',\n",
       "  'get_output_file_attributes',\n",
       "  'inFiles',\n",
       "  'endTime',\n",
       "  'pilotClosed',\n",
       "  'preparatorTime',\n",
       "  'metaData',\n",
       "  'jobParamsExtForLog',\n",
       "  'startTime',\n",
       "  'events',\n",
       "  'add_out_file',\n",
       "  'propagatorTime',\n",
       "  'subStatus',\n",
       "  'attemptNr',\n",
       "  'get_input_file_attributes',\n",
       "  'add_in_file',\n",
       "  'status',\n",
       "  'taskID',\n",
       "  'jobParamsExtForOutput',\n",
       "  'jobsetID',\n",
       "  'stagerTime',\n",
       "  'outputFilesToReport',\n",
       "  'get_logfile_info',\n",
       "  'PandaID',\n",
       "  'currentPriority',\n",
       "  'force_update',\n",
       "  'set_one_attribute'],\n",
       " 'function_args': ['skip_ready',\n",
       "  'workspec_list',\n",
       "  'strip',\n",
       "  'error_dialog',\n",
       "  'filespec',\n",
       "  'event_spec',\n",
       "  'force',\n",
       "  'attrs',\n",
       "  'job_status',\n",
       "  'data',\n",
       "  'attr',\n",
       "  'use_post_zipping',\n",
       "  'skip_done',\n",
       "  'in_files',\n",
       "  'group_status',\n",
       "  'group_id',\n",
       "  'error_code',\n",
       "  'max_events',\n",
       "  'zip_filespec',\n",
       "  'id_map',\n",
       "  'value'],\n",
       " 'docstrings': []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = extract_code_elements(sample['context'][0])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98038/98038 [1:19:45<00:00, 20.49it/s]\n"
     ]
    }
   ],
   "source": [
    "symm_preds, asymm_preds, gts = [], [], []\n",
    "\n",
    "for sample in tqdm(raw_samples):\n",
    "    with torch.inference_mode():\n",
    "        nl_embedding = model.encode(sample['next_line'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "\n",
    "        # raw code\n",
    "        code_embedding = model.encode(sample['code'], convert_to_tensor=True, convert_to_numpy=False)\n",
    "\n",
    "        # only keywords\n",
    "        new_context = []\n",
    "        for c in sample['context']:\n",
    "            code_meta = extract_code_elements(c)\n",
    "            s = \"\"\n",
    "            s += \" \".join(code_meta['class_names']) + \" \"\n",
    "            s += \" \".join(code_meta['function_names']) + \" \"\n",
    "            s += \" \".join(code_meta['docstrings']) + \" \"\n",
    "            \n",
    "            new_context.append(s)\n",
    "        context_embedding = model.encode(new_context, batch_size=16, convert_to_tensor=True, convert_to_numpy=False)\n",
    "        \n",
    "        symm_dist = nl_embedding @ context_embedding.T\n",
    "        asymm_dist = code_embedding @ context_embedding.T\n",
    "        \n",
    "        symm_preds.append(symm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        asymm_preds.append(asymm_dist.argsort(descending=True).detach().cpu().numpy())\n",
    "        \n",
    "        gts.append(sample['golden_snippet_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def synchronize(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "    def __init__(self):\n",
      "    def get_elapsed_time(self):\n",
      "    def get_elapsed_time_in_sec(self, precise=False):\n",
      "    def reset(self):\n",
      "    def __init__(self):\n",
      "    def __getitem__(self, item):\n",
      "    def __setitem__(self, item, value):\n",
      "    def __contains__(self, item):\n",
      "    def acquire(self):\n",
      "    def release(self):\n",
      "    def iteritems(self):\n",
      "    def __init__(cls, *args,**kwargs):\n",
      "    def __call__(cls, *args, **kwargs):\n",
      "    def __init__(cls, *args,**kwargs):\n",
      "    def __call__(cls, *args, **kwargs):\n",
      "def enable_memory_profiling():\n",
      "def setup_logger(name=None):\n",
      "def make_logger(tmp_log, token=None, method_name=None, hook=None):\n",
      "def dump_error_message(tmp_log, err_str=None, no_message=False):\n",
      "def sleep(interval, stop_event, randomize=True):\n",
      "def make_pool_file_catalog(jobspec_list):\n",
      "def calc_adler32(file_name):\n",
      "def get_output_file_report(jobspec):\n",
      "def create_shards(input_list, size):\n",
      "def update_job_attributes_with_workers(map_type, jobspec_list, workspec_list, files_to_stage_out_list,\n",
      "                                       events_to_update_list):\n",
      "def do_log_rollover():\n",
      "def get_stopwatch():\n",
      "def get_global_dict():\n",
      "def get_file_lock(file_name, lock_interval):\n",
      "def convert_phrase_to_key(key_phrase):\n",
      "def encrypt_string(key_phrase, plain_text):\n",
      "def decrypt_string(key_phrase, cipher_text):\n",
      "def set_file_permission(path):\n",
      "def get_queues_config_url():\n",
      "def get_unique_queue_name(queue_name, resource_type):\n",
      "def dynamic_plugin_change():\n",
      "    def set_attributes(self, attributes):\n",
      "    def _asdict(self):\n",
      "def make_choice_list(pdpm={}, default=None):\n",
      "class StopWatch(object):\n",
      "class MapWithLock(object):\n",
      "class SingletonWithID(type):\n",
      "class SingletonWithThreadAndID(type):\n",
      "class DictTupleHybrid(tuple):\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context - only keywords and dosctrings\n",
      "symm\n",
      "accuracy@1: 0.72368\n",
      "accuracy@3: 0.89186\n",
      "accuracy@5: 0.96084\n",
      "assym\n",
      "accuracy@1: 0.14054\n",
      "accuracy@3: 0.45268\n",
      "accuracy@5: 0.78271\n"
     ]
    }
   ],
   "source": [
    "print('context - only keywords and dosctrings')\n",
    "print('symm')\n",
    "print_metrics(symm_preds, gts)\n",
    "print('assym')\n",
    "print_metrics(asymm_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del symm_dist, asymm_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
