{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de90c573-7967-4f1e-8e5c-66f5de619d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# # Load the pre-trained GPT-Neo model tokenizer (Replace with smaller or larger version)\n",
    "# model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
    "\n",
    "# # Load the text dataset\n",
    "# with open('text.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # Tokenize the text dataset\n",
    "# encoding = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# # Fine-tune the model on the text dataset\n",
    "# model.train()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "# for i in range(100):\n",
    "#     loss = model(encoding, labels=encoding)[0]\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(f'Epoch {i+1}, Loss: {loss.item()}')\n",
    "\n",
    "# # Save the fine-tuned model and tokenizer\n",
    "# model.save_pretrained('fine-tuned-gpt-neo')\n",
    "# tokenizer.save_pretrained('fine-tuned-gpt-neo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54dd338-c88b-450e-b065-ffc8c84a5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import sys\n",
    "sys.path.insert(1, '../repos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e18f6e4-398d-4de4-a7a6-278e9dee68f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/dimitriishh/workspace/envs/base_env/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from repobench.archive_data.utils import load_data\n",
    "from repobench.evaluation.metrics import accuracy_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a8e3be-28e6-44db-add0-d8000efda389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "data = load_data(split='test', task='retrieval', language='python', settings=['cross_file_first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76f1b28-31e9-4922-a067-7666a8c7c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1dea2b-60e1-42a2-b1e1-24e8c44e43a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'jtydhr88/sd-webui-txt-img-to-3d-model',\n",
       " 'file_path': 'shap_e/models/nn/ops.py',\n",
       " 'context': ['class AttrDict(OrderedDict[K, V], Generic[K, V]):\\n    \"\"\"\\n    An attribute dictionary that automatically handles nested keys joined by \"/\".\\n\\n    Originally copied from: https://stackoverflow.com/questions/3031219/recursively-access-dict-via-attributes-as-well-as-index-access\\n    \"\"\"\\n\\n    MARKER = object()\\n\\n    # pylint: disable=super-init-not-called\\n    def __init__(self, *args, **kwargs):\\n        if len(args) == 0:\\n            for key, value in kwargs.items():\\n                self.__setitem__(key, value)\\n        else:\\n            assert len(args) == 1\\n            assert isinstance(args[0], (dict, AttrDict))\\n            for key, value in args[0].items():\\n                self.__setitem__(key, value)\\n\\n    def __contains__(self, key):\\n        if \"/\" in key:\\n            keys = key.split(\"/\")\\n            key, next_key = keys[0], \"/\".join(keys[1:])\\n            return key in self and next_key in self[key]\\n        return super(AttrDict, self).__contains__(key)\\n\\n    def __setitem__(self, key, value):\\n        if \"/\" in key:\\n            keys = key.split(\"/\")\\n            key, next_key = keys[0], \"/\".join(keys[1:])\\n            if key not in self:\\n                self[key] = AttrDict()\\n            self[key].__setitem__(next_key, value)\\n            return\\n\\n        if isinstance(value, dict) and not isinstance(value, AttrDict):\\n            value = AttrDict(**value)\\n        if isinstance(value, list):\\n            value = [AttrDict(val) if isinstance(val, dict) else val for val in value]\\n        super(AttrDict, self).__setitem__(key, value)\\n\\n    def __getitem__(self, key):\\n        if \"/\" in key:\\n            keys = key.split(\"/\")\\n            key, next_key = keys[0], \"/\".join(keys[1:])\\n            val = self[key]\\n            if not isinstance(val, AttrDict):\\n                raise ValueError\\n            return val.__getitem__(next_key)\\n\\n        return self.get(key, None)\\n\\n    def all_keys(\\n        self,\\n        leaves_only: bool = False,\\n        parent: Optional[str] = None,\\n    ) -> List[str]:\\n        keys = []\\n        for key in self.keys():\\n            cur = key if parent is None else f\"{parent}/{key}\"\\n            if not leaves_only or not isinstance(self[key], dict):\\n                keys.append(cur)\\n            if isinstance(self[key], dict):\\n                keys.extend(self[key].all_keys(leaves_only=leaves_only, parent=cur))\\n        return keys\\n\\n    def dumpable(self, strip=True):\\n        \"\"\"\\n        Casts into OrderedDict and removes internal attributes\\n        \"\"\"\\n\\n        def _dump(val):\\n            if isinstance(val, AttrDict):\\n                return val.dumpable()\\n            elif isinstance(val, list):\\n                return [_dump(v) for v in val]\\n            return val\\n\\n        if strip:\\n            return {k: _dump(v) for k, v in self.items() if not k.startswith(\"_\")}\\n        return {k: _dump(v if not k.startswith(\"_\") else repr(v)) for k, v in self.items()}\\n\\n    def map(\\n        self,\\n        map_fn: Callable[[Any, Any], Any],\\n        should_map: Optional[Callable[[Any, Any], bool]] = None,\\n    ) -> \"AttrDict\":\\n        \"\"\"\\n        Creates a copy of self where some or all values are transformed by\\n        map_fn.\\n\\n        :param should_map: If provided, only those values that evaluate to true\\n            are converted; otherwise, all values are mapped.\\n        \"\"\"\\n\\n        def _apply(key, val):\\n            if isinstance(val, AttrDict):\\n                return val.map(map_fn, should_map)\\n            elif should_map is None or should_map(key, val):\\n                return map_fn(key, val)\\n            return val\\n\\n        return AttrDict({k: _apply(k, v) for k, v in self.items()})\\n\\n    def __eq__(self, other):\\n        return self.keys() == other.keys() and all(self[k] == other[k] for k in self.keys())\\n\\n    def combine(\\n        self,\\n        other: Dict[str, Any],\\n        combine_fn: Callable[[Optional[Any], Optional[Any]], Any],\\n    ) -> \"AttrDict\":\\n        \"\"\"\\n        Some values may be missing, but the dictionary structures must be the\\n        same.\\n\\n        :param combine_fn: a (possibly non-commutative) function to combine the\\n            values\\n        \"\"\"\\n\\n        def _apply(val, other_val):\\n            if val is not None and isinstance(val, AttrDict):\\n                assert isinstance(other_val, AttrDict)\\n                return val.combine(other_val, combine_fn)\\n            return combine_fn(val, other_val)\\n\\n        # TODO nit: this changes the ordering..\\n        keys = self.keys() | other.keys()\\n        return AttrDict({k: _apply(self[k], other[k]) for k in keys})\\n\\n    __setattr__, __getattr__ = __setitem__, __getitem__',\n",
       "  'class MetaModule(nn.Module):\\n    \"\"\"\\n    Base class for PyTorch meta-learning modules. These modules accept an\\n    additional argument `params` in their `forward` method.\\n\\n    Notes\\n    -----\\n    Objects inherited from `MetaModule` are fully compatible with PyTorch\\n    modules from `torch.nn.Module`. The argument `params` is a dictionary of\\n    tensors, with full support of the computation graph (for differentiation).\\n\\n    Based on SIREN\\'s torchmeta with some additional features/changes.\\n\\n    All meta weights must not have the batch dimension, as they are later tiled\\n    to the given batch size after unsqueezing the first dimension (e.g. a\\n    weight of dimension [d_out x d_in] is tiled to have the dimension [batch x\\n    d_out x d_in]).  Requiring all meta weights to have a batch dimension of 1\\n    (e.g. [1 x d_out x d_in] from the earlier example) could be a more natural\\n    choice, but this results in silent failures.\\n    \"\"\"\\n\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self._meta_state_dict = set()\\n        self._meta_params = set()\\n\\n    def register_meta_buffer(self, name: str, param: nn.Parameter):\\n        \"\"\"\\n        Registers a trainable or nontrainable parameter as a meta buffer. This\\n        can be later retrieved by meta_state_dict\\n        \"\"\"\\n        self.register_buffer(name, param)\\n        self._meta_state_dict.add(name)\\n\\n    def register_meta_parameter(self, name: str, parameter: nn.Parameter):\\n        \"\"\"\\n        Registers a meta parameter so it is included in named_meta_parameters\\n        and meta_state_dict.\\n        \"\"\"\\n        self.register_parameter(name, parameter)\\n        self._meta_params.add(name)\\n        self._meta_state_dict.add(name)\\n\\n    def register_meta(self, name: str, parameter: nn.Parameter, trainable: bool = True):\\n        if trainable:\\n            self.register_meta_parameter(name, parameter)\\n        else:\\n            self.register_meta_buffer(name, parameter)\\n\\n    def register(self, name: str, parameter: nn.Parameter, meta: bool, trainable: bool = True):\\n        if meta:\\n            if trainable:\\n                self.register_meta_parameter(name, parameter)\\n            else:\\n                self.register_meta_buffer(name, parameter)\\n        else:\\n            if trainable:\\n                self.register_parameter(name, parameter)\\n            else:\\n                self.register_buffer(name, parameter)\\n\\n    def named_meta_parameters(self, prefix=\"\", recurse=True):\\n        \"\"\"\\n        Returns an iterator over all the names and meta parameters\\n        \"\"\"\\n\\n        def meta_iterator(module):\\n            meta = module._meta_params if isinstance(module, MetaModule) else set()\\n            for name, param in module._parameters.items():\\n                if name in meta:\\n                    yield name, param\\n\\n        gen = self._named_members(\\n            meta_iterator,\\n            prefix=prefix,\\n            recurse=recurse,\\n        )\\n        for name, param in gen:\\n            yield name, param\\n\\n    def named_nonmeta_parameters(self, prefix=\"\", recurse=True):\\n        def _iterator(module):\\n            meta = module._meta_params if isinstance(module, MetaModule) else set()\\n            for name, param in module._parameters.items():\\n                if name not in meta:\\n                    yield name, param\\n\\n        gen = self._named_members(\\n            _iterator,\\n            prefix=prefix,\\n            recurse=recurse,\\n        )\\n        for name, param in gen:\\n            yield name, param\\n\\n    def nonmeta_parameters(self, prefix=\"\", recurse=True):\\n        for _, param in self.named_nonmeta_parameters(prefix=prefix, recurse=recurse):\\n            yield param\\n\\n    def meta_state_dict(self, prefix=\"\", recurse=True):\\n        \"\"\"\\n        Returns an iterator over all the names and meta parameters/buffers.\\n\\n        One difference between module.state_dict() is that this preserves\\n        requires_grad, because we may want to compute the gradient w.r.t. meta\\n        buffers, but don\\'t necessarily update them automatically.\\n        \"\"\"\\n\\n        def meta_iterator(module):\\n            meta = module._meta_state_dict if isinstance(module, MetaModule) else set()\\n            for name, param in itertools.chain(module._buffers.items(), module._parameters.items()):\\n                if name in meta:\\n                    yield name, param\\n\\n        gen = self._named_members(\\n            meta_iterator,\\n            prefix=prefix,\\n            recurse=recurse,\\n        )\\n        return dict(gen)\\n\\n    def update(self, params=None):\\n        \"\"\"\\n        Updates the parameter list before the forward prop so that if `params`\\n        is None or doesn\\'t have a certain key, the module uses the default\\n        parameter/buffer registered in the module.\\n        \"\"\"\\n        if params is None:\\n            params = AttrDict()\\n        params = AttrDict(params)\\n        named_params = set([name for name, _ in self.named_parameters()])\\n        for name, param in self.named_parameters():\\n            params.setdefault(name, param)\\n        for name, param in self.state_dict().items():\\n            if name not in named_params:\\n                params.setdefault(name, param)\\n        return params',\n",
       "  'def subdict(dictionary, key=None):\\n    if dictionary is None:\\n        return None\\n    if (key is None) or (key == \"\"):\\n        return dictionary\\n    key_re = re.compile(r\"^{0}\\\\.(.+)\".format(re.escape(key)))\\n    return AttrDict(\\n        OrderedDict(\\n            (key_re.sub(r\"\\\\1\", k), value)\\n            for (k, value) in dictionary.items()\\n            if key_re.match(k) is not None\\n        )\\n    )',\n",
       "  'def sample_and_group(\\n    npoint,\\n    radius,\\n    nsample,\\n    xyz,\\n    points,\\n    returnfps=False,\\n    deterministic=False,\\n    fps_method: str = \"fps\",\\n):\\n    \"\"\"\\n    Input:\\n        npoint:\\n        radius:\\n        nsample:\\n        xyz: input points position data, [B, N, 3]\\n        points: input points data, [B, N, D]\\n    Return:\\n        new_xyz: sampled points position data, [B, npoint, nsample, 3]\\n        new_points: sampled points data, [B, npoint, nsample, 3+D]\\n    \"\"\"\\n    B, N, C = xyz.shape\\n    S = npoint\\n    if fps_method == \"fps\":\\n        fps_idx = farthest_point_sample(xyz, npoint, deterministic=deterministic)  # [B, npoint, C]\\n    elif fps_method == \"first\":\\n        fps_idx = torch.arange(npoint)[None].repeat(B, 1)\\n    else:\\n        raise ValueError(f\"Unknown FPS method: {fps_method}\")\\n    new_xyz = index_points(xyz, fps_idx)\\n    idx = query_ball_point(radius, nsample, xyz, new_xyz)\\n    grouped_xyz = index_points(xyz, idx)  # [B, npoint, nsample, C]\\n    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\\n\\n    if points is not None:\\n        grouped_points = index_points(points, idx)\\n        new_points = torch.cat(\\n            [grouped_xyz_norm, grouped_points], dim=-1\\n        )  # [B, npoint, nsample, C+D]\\n    else:\\n        new_points = grouped_xyz_norm\\n    if returnfps:\\n        return new_xyz, new_points, grouped_xyz, fps_idx\\n    else:\\n        return new_xyz, new_points',\n",
       "  'def sample_and_group_all(xyz, points):\\n    \"\"\"\\n    Input:\\n        xyz: input points position data, [B, N, 3]\\n        points: input points data, [B, N, D]\\n    Return:\\n        new_xyz: sampled points position data, [B, 1, 3]\\n        new_points: sampled points data, [B, 1, N, 3+D]\\n    \"\"\"\\n    device = xyz.device\\n    B, N, C = xyz.shape\\n    new_xyz = torch.zeros(B, 1, C).to(device)\\n    grouped_xyz = xyz.view(B, 1, N, C)\\n    if points is not None:\\n        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\\n    else:\\n        new_points = grouped_xyz\\n    return new_xyz, new_points'],\n",
       " 'import_statement': 'import math\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom typing import List, Optional, Tuple, Union\\nfrom shap_e.util.collections import AttrDict\\nfrom .meta import MetaModule, subdict\\nfrom .pointnet2_utils import sample_and_group, sample_and_group_all',\n",
       " 'code': '        nn.init.constant_(affine.bias, 0.0)\\n\\n\\ndef siren_init_30(affine, init_scale: float = 1.0):\\n    siren_init(affine, coeff=30.0, init_scale=init_scale)\\n\\n\\ndef std_init(affine, init_scale: float = 1.0):\\n    n_in = affine.weight.shape[1]\\n    stddev = init_scale / math.sqrt(n_in)\\n    nn.init.normal_(affine.weight, std=stddev)\\n    if affine.bias is not None:\\n        nn.init.constant_(affine.bias, 0.0)\\n\\n\\ndef mlp_init(affines, init: Optional[str] = None, init_scale: float = 1.0):\\n    if init == \"siren30\":\\n        for idx, affine in enumerate(affines):\\n            init = siren_init_first_layer if idx == 0 else siren_init_30\\n            init(affine, init_scale=init_scale)\\n    elif init == \"siren\":\\n        for idx, affine in enumerate(affines):\\n            init = siren_init_first_layer if idx == 0 else siren_init\\n            init(affine, init_scale=init_scale)\\n    elif init is None:\\n        for affine in affines:\\n            std_init(affine, init_scale=init_scale)\\n    else:\\n        raise NotImplementedError(init)\\n\\n\\nclass MetaLinear(MetaModule):\\n    def __init__(\\n        self,\\n        n_in,\\n        n_out,\\n        bias: bool = True,\\n        meta_scale: bool = True,\\n        meta_shift: bool = True,\\n        meta_proj: bool = False,\\n        meta_bias: bool = False,\\n        trainable_meta: bool = False,\\n        **kwargs,\\n    ):\\n        super().__init__()\\n        # n_in, n_out, bias=bias)\\n        register_meta_fn = (\\n            self.register_meta_parameter if trainable_meta else self.register_meta_buffer\\n        )\\n        if meta_scale:\\n            register_meta_fn(\"scale\", nn.Parameter(torch.ones(n_out, **kwargs)))\\n        if meta_shift:\\n            register_meta_fn(\"shift\", nn.Parameter(torch.zeros(n_out, **kwargs)))\\n\\n        register_proj_fn = self.register_parameter if not meta_proj else register_meta_fn\\n        register_proj_fn(\"weight\", nn.Parameter(torch.empty((n_out, n_in), **kwargs)))\\n\\n        if not bias:\\n            self.register_parameter(\"bias\", None)\\n        else:\\n            register_bias_fn = self.register_parameter if not meta_bias else register_meta_fn\\n            register_bias_fn(\"bias\", nn.Parameter(torch.empty(n_out, **kwargs)))\\n\\n        self.reset_parameters()\\n\\n    def reset_parameters(self) -> None:\\n\\n        # from https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\\n\\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\\n        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\\n        # https://github.com/pytorch/pytorch/issues/57109\\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\\n        if self.bias is not None:\\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\\n            nn.init.uniform_(self.bias, -bound, bound)\\n\\n    def _bcast(self, op, left, right):\\n        if right.ndim == 2:\\n            # Has dimension [batch x d_output]\\n            right = right.unsqueeze(1)\\n        return op(left, right)\\n\\n    def forward(self, x, params=None):\\n        params = self.update(params)\\n\\n        batch_size, *shape, d_in = x.shape\\n        x = x.view(batch_size, -1, d_in)\\n\\n        if params.weight.ndim == 2:\\n            h = torch.einsum(\"bni,oi->bno\", x, params.weight)\\n        elif params.weight.ndim == 3:\\n            h = torch.einsum(\"bni,boi->bno\", x, params.weight)\\n\\n        if params.bias is not None:\\n            h = self._bcast(torch.add, h, params.bias)\\n\\n        if params.scale is not None:\\n            h = self._bcast(torch.mul, h, params.scale)\\n\\n        if params.shift is not None:\\n            h = self._bcast(torch.add, h, params.shift)\\n\\n        h = h.view(batch_size, *shape, -1)\\n        return h\\n\\n\\ndef Conv(n_dim, d_in, d_out, kernel, stride=1, padding=0, dilation=1, **kwargs):\\n    cls = {\\n        1: nn.Conv1d,\\n        2: nn.Conv2d,\\n        3: nn.Conv3d,\\n    }[n_dim]\\n    return cls(d_in, d_out, kernel, stride=stride, padding=padding, dilation=dilation, **kwargs)\\n\\n\\ndef flatten(x):\\n    batch_size, *shape, n_channels = x.shape\\n    n_ctx = np.prod(shape)\\n',\n",
       " 'next_line': '    return x.view(batch_size, n_ctx, n_channels), AttrDict(',\n",
       " 'gold_snippet_index': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2dd29d3-0bc0-49e6-a6d2-d0de86a56395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d48f282-ed58-470f-8c83-49479ccb8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "187904d8-6765-489d-8e76-aeb879cdae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d47269-b6fa-4006-a985-e776b498c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a76e78ae-7935-4aab-b06c-f1b109433344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gold_snippets = [data[i]['gold_snippet_index'] for i in range(10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac1bb9-3aec-4310-a428-75cd32e6ef50",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "128b1a13-7102-4369-bda4-4e9c5f6172ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rand = []\n",
    "for i in range(10000):\n",
    "    y_pred_rand.append(np.random.permutation(range(len(data[i]['context']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ab6f7f-5097-4421-8799-f6ce5c4b2b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy@1: 0.1549\n",
      "accuracy@3: 0.4698\n",
      "accuracy@5: 0.7832\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 6, 2):\n",
    "    print(f'accuracy@{k}: {accuracy_at_k(y_pred_rand, gold_snippets, k=k)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebd346-af7a-48fe-b22b-0b88e9847130",
   "metadata": {},
   "source": [
    "## Jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63611fbb-885b-460a-a6ed-8f55b842118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2): \n",
    "    doc1 = set(doc1)\n",
    "    doc2 = set(doc2)\n",
    "    intersection = doc1.intersection(doc2)\n",
    "    union = doc1.union(doc2)\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e415bf1-d930-42a8-9824-519544a1fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_jac = []\n",
    "for i in range(10000):\n",
    "    dists = []\n",
    "    ids_next_line = tokenizer(data[i]['next_line']).input_ids\n",
    "    for num, snippet in enumerate(data[i]['context']):\n",
    "        dists.append(jaccard_similarity(tokenizer(snippet).input_ids, ids_next_line))\n",
    "    y_pred_jac.append(np.argsort(dists)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dda9d149-8964-4f04-b7a2-4a33dcef1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy@1: 0.3972\n",
      "accuracy@3: 0.6787\n",
      "accuracy@5: 0.8817\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 6, 2):\n",
    "    print(f'accuracy@{k}: {accuracy_at_k(y_pred_jac, gold_snippets, k=k)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7cb0c47-d013-43da-b7c6-0d0586958bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Edit similarity\n",
    "y_pred_es = []\n",
    "from fuzzywuzzy import fuzz\n",
    "for i in range(10000):\n",
    "    dists = []\n",
    "    ids_next_line = tokenizer(data[i]['next_line']).input_ids\n",
    "    for num, snippet in enumerate(data[i]['context']):\n",
    "        dists.append(fuzz.ratio(tokenizer(snippet).input_ids, ids_next_line))\n",
    "    y_pred_es.append(np.argsort(dists)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bb36fe5-e1e1-420e-b858-1a03b4132970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy@1: 0.18\n",
      "accuracy@3: 0.4771\n",
      "accuracy@5: 0.7793\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 6, 2):\n",
    "    print(f'accuracy@{k}: {accuracy_at_k(y_pred_es, gold_snippets, k=k)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c158ff-06ef-43b8-a29f-b61ddeb114cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"next_line\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0ff8b-ae9a-4e3a-9f8b-4b40581e050c",
   "metadata": {},
   "source": [
    "## all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd725db9-a827-4a12-bd78-c81f46a88d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "y_pred_all_mini = []\n",
    "for i in range(10000):\n",
    "    # Corpus with example sentences\n",
    "    corpus_embeddings = embedder.encode(data[i][\"context\"], convert_to_tensor=True)\n",
    "    # Query sentences:\n",
    "    query_embedding = embedder.encode(data[i][\"next_line\"], convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    y_pred_all_mini.append(torch.argsort(cos_scores, descending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "710eb645-150b-4087-8318-d723118e2557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy@1: 0.7491\n",
      "accuracy@3: 0.9391\n",
      "accuracy@5: 0.9824\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 6, 2):\n",
    "    print(f'accuracy@{k}: {accuracy_at_k(y_pred_all_mini, gold_snippets, k=k)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc87841-1359-46fc-9c38-49fed68e3d5f",
   "metadata": {},
   "source": [
    "## UnixCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d2ec1da-d0f7-4fce-8029-1495fccaa8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 384])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.encode(data[i][\"context\"], convert_to_tensor=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d6f54ee-1bd1-4323-97f9-30e3670521e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.encode(data[i][\"next_line\"], convert_to_tensor=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24d66dcc-8177-41b9-a663-b5f1884577ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniXcoder(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(51416, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51416, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unixcoder import UniXcoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "737c08a4-d8b6-44c2-a649-fd73bb96f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unixcoder_encode(snippet):\n",
    "    tokens_ids = model.tokenize([snippet],max_length=512,mode=\"<encoder-only>\")\n",
    "    return torch.tensor(model(torch.tensor(tokens_ids).to(device))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18e507ec-e532-43a4-b094-ea7a40a6881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113521/2425828202.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(model(torch.tensor(tokens_ids).to(device))[1])\n"
     ]
    }
   ],
   "source": [
    "y_pred_unixcoder = []\n",
    "for i in range(10000):\n",
    "    # Corpus with example sentences\n",
    "    corpus_embeddings = torch.cat(list(map(unixcoder_encode, data[i][\"context\"])))\n",
    "    # Query sentences:\n",
    "    query_embedding = unixcoder_encode(data[i][\"next_line\"])\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    y_pred_unixcoder.append(torch.argsort(cos_scores, descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0e89d54-c8aa-44cc-b2f7-c3c4dae1631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy@1: 0.6583\n",
      "accuracy@3: 0.8884\n",
      "accuracy@5: 0.9702\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 6, 2):\n",
    "    print(f'accuracy@{k}: {accuracy_at_k(y_pred_all_mini[10000:], gold_snippets, k=k)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea95585-7132-4636-a113-ac2c72f0a518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
